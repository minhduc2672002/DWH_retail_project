[2024-03-23T15:10:38.325+0000] {processor.py:161} INFO - Started process (PID=167) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:10:38.329+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:10:38.335+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:10:38.334+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:10:38.349+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:10:38.341+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:10:38.354+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:10:38.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.074 seconds
[2024-03-23T15:11:08.602+0000] {processor.py:161} INFO - Started process (PID=183) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:11:08.605+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:11:08.611+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:11:08.610+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:11:08.623+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:11:08.620+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:11:08.625+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:11:08.656+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T15:11:38.859+0000] {processor.py:161} INFO - Started process (PID=199) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:11:38.861+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:11:38.868+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:11:38.866+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:11:38.881+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:11:38.877+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:11:38.883+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:11:38.928+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.077 seconds
[2024-03-23T15:12:09.118+0000] {processor.py:161} INFO - Started process (PID=215) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:12:09.120+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:12:09.127+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:12:09.125+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:12:09.135+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:12:09.133+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:12:09.137+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:12:09.170+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T15:12:39.364+0000] {processor.py:161} INFO - Started process (PID=231) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:12:39.370+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:12:39.380+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:12:39.379+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:12:39.391+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:12:39.388+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:12:39.393+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:12:39.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.094 seconds
[2024-03-23T15:13:09.634+0000] {processor.py:161} INFO - Started process (PID=247) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:13:09.636+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:13:09.640+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:13:09.640+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:13:09.652+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:13:09.648+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:13:09.653+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:13:09.677+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.052 seconds
[2024-03-23T15:13:39.849+0000] {processor.py:161} INFO - Started process (PID=263) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:13:39.851+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:13:39.857+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:13:39.856+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:13:39.871+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:13:39.866+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:13:39.873+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:13:39.909+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T15:14:10.057+0000] {processor.py:161} INFO - Started process (PID=279) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:14:10.059+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:14:10.065+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:14:10.064+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:14:10.077+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:14:10.073+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:14:10.079+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:14:10.132+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.083 seconds
[2024-03-23T15:14:40.287+0000] {processor.py:161} INFO - Started process (PID=295) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:14:40.291+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:14:40.294+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:14:40.294+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:14:40.308+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:14:40.304+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:14:40.310+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:14:40.345+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T15:15:10.499+0000] {processor.py:161} INFO - Started process (PID=311) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:15:10.500+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:15:10.503+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:15:10.503+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:15:10.511+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:15:10.509+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:15:10.512+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:15:10.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T15:15:40.797+0000] {processor.py:161} INFO - Started process (PID=327) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:15:40.799+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:15:40.803+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:15:40.802+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:15:40.814+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:15:40.811+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:15:40.817+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:15:40.851+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T15:16:11.037+0000] {processor.py:161} INFO - Started process (PID=343) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:16:11.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:16:11.042+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:16:11.042+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:16:11.050+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:16:11.048+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:16:11.051+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:16:11.090+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.056 seconds
[2024-03-23T15:16:41.271+0000] {processor.py:161} INFO - Started process (PID=359) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:16:41.273+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:16:41.276+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:16:41.276+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:16:41.284+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:16:41.283+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:16:41.285+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:16:41.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.052 seconds
[2024-03-23T15:17:11.583+0000] {processor.py:161} INFO - Started process (PID=375) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:17:11.586+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:17:11.593+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:17:11.591+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:17:11.607+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:17:11.603+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:17:11.609+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:17:11.644+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T15:17:41.851+0000] {processor.py:161} INFO - Started process (PID=391) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:17:41.853+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:17:41.856+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:17:41.855+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:17:41.866+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:17:41.864+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:17:41.867+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:17:41.895+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.047 seconds
[2024-03-23T15:18:12.062+0000] {processor.py:161} INFO - Started process (PID=407) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:18:12.063+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:18:12.067+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:18:12.066+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:18:12.074+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:18:12.072+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:18:12.075+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:18:12.101+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.042 seconds
[2024-03-23T15:18:42.271+0000] {processor.py:161} INFO - Started process (PID=423) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:18:42.273+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:18:42.279+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:18:42.277+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:18:42.288+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:18:42.286+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:18:42.290+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:18:42.325+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T15:19:12.531+0000] {processor.py:161} INFO - Started process (PID=439) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:19:12.533+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:19:12.536+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:19:12.535+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:19:12.546+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:19:12.544+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:19:12.548+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:19:12.591+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T15:19:42.783+0000] {processor.py:161} INFO - Started process (PID=455) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:19:42.785+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:19:42.790+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:19:42.789+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:19:42.803+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:19:42.801+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:19:42.805+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:19:42.832+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.057 seconds
[2024-03-23T15:20:13.024+0000] {processor.py:161} INFO - Started process (PID=471) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:20:13.026+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:20:13.030+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:20:13.029+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:20:13.038+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:20:13.036+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:20:13.039+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:20:13.068+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.049 seconds
[2024-03-23T15:20:43.259+0000] {processor.py:161} INFO - Started process (PID=487) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:20:43.261+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:20:43.272+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:20:43.270+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:20:43.283+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:20:43.279+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:20:43.285+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:20:43.326+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.073 seconds
[2024-03-23T15:21:13.519+0000] {processor.py:161} INFO - Started process (PID=503) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:21:13.522+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:21:13.528+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:21:13.527+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:21:13.540+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:21:13.536+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:21:13.542+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:21:13.568+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.055 seconds
[2024-03-23T15:21:43.744+0000] {processor.py:161} INFO - Started process (PID=519) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:21:43.747+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:21:43.754+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:21:43.752+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:21:43.768+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:21:43.763+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:21:43.770+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:21:43.791+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.057 seconds
[2024-03-23T15:22:14.000+0000] {processor.py:161} INFO - Started process (PID=534) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:22:14.003+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:22:14.009+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:22:14.008+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:22:14.018+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:22:14.015+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:22:14.020+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:22:14.044+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.052 seconds
[2024-03-23T15:22:44.260+0000] {processor.py:161} INFO - Started process (PID=550) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:22:44.263+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:22:44.270+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:22:44.269+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:22:44.280+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:22:44.278+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:22:44.281+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:22:44.317+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T15:23:14.458+0000] {processor.py:161} INFO - Started process (PID=566) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:23:14.459+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:23:14.463+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:23:14.463+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:23:14.473+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:23:14.471+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:23:14.475+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:23:14.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T15:23:44.731+0000] {processor.py:161} INFO - Started process (PID=582) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:23:44.734+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:23:44.742+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:23:44.741+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:23:44.751+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:23:44.748+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:23:44.752+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:23:44.786+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T15:24:14.988+0000] {processor.py:161} INFO - Started process (PID=599) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:24:14.990+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:24:14.997+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:24:14.996+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:24:15.011+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:24:15.009+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:24:15.013+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:24:15.039+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T15:24:45.247+0000] {processor.py:161} INFO - Started process (PID=615) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:24:45.249+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:24:45.256+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:24:45.255+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:24:45.267+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:24:45.264+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:24:45.268+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:24:45.300+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T15:25:15.497+0000] {processor.py:161} INFO - Started process (PID=631) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:25:15.500+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:25:15.505+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:25:15.504+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:25:15.515+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:25:15.512+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:25:15.517+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:25:15.554+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T15:25:45.782+0000] {processor.py:161} INFO - Started process (PID=646) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:25:45.784+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:25:45.791+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:25:45.790+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:25:45.802+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:25:45.798+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:25:45.804+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:25:45.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T15:26:16.047+0000] {processor.py:161} INFO - Started process (PID=662) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:26:16.048+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:26:16.052+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:26:16.051+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:26:16.060+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:26:16.058+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:26:16.061+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:26:16.099+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.056 seconds
[2024-03-23T15:26:46.315+0000] {processor.py:161} INFO - Started process (PID=678) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:26:46.318+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:26:46.325+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:26:46.324+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:26:46.340+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:26:46.335+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:26:46.342+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:26:46.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.071 seconds
[2024-03-23T15:27:16.582+0000] {processor.py:161} INFO - Started process (PID=694) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:27:16.585+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:27:16.591+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:27:16.590+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:27:16.606+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:27:16.602+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:27:16.608+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:27:16.640+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T15:27:46.834+0000] {processor.py:161} INFO - Started process (PID=710) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:27:46.836+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:27:46.842+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:27:46.841+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:27:46.853+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:27:46.850+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:27:46.856+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:27:46.898+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.072 seconds
[2024-03-23T15:28:17.087+0000] {processor.py:161} INFO - Started process (PID=726) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:28:17.089+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:28:17.092+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:28:17.092+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:28:17.106+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:28:17.102+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:28:17.108+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:28:17.145+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T15:28:47.357+0000] {processor.py:161} INFO - Started process (PID=742) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:28:47.359+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:28:47.365+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:28:47.364+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:28:47.374+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:28:47.371+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:28:47.375+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:28:47.408+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T15:29:17.606+0000] {processor.py:161} INFO - Started process (PID=758) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:29:17.607+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:29:17.611+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:29:17.610+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:29:17.624+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:29:17.620+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:29:17.626+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:29:17.661+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T15:29:47.866+0000] {processor.py:161} INFO - Started process (PID=774) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:29:47.868+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:29:47.874+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:29:47.874+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:29:47.884+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:29:47.882+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:29:47.885+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:29:47.924+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T15:30:18.131+0000] {processor.py:161} INFO - Started process (PID=790) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:30:18.133+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:30:18.138+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:30:18.137+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:30:18.148+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:30:18.146+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:30:18.150+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:30:18.194+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.069 seconds
[2024-03-23T15:30:48.439+0000] {processor.py:161} INFO - Started process (PID=806) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:30:48.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:30:48.446+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:30:48.445+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:30:48.458+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:30:48.455+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:30:48.459+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:30:48.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T15:31:18.667+0000] {processor.py:161} INFO - Started process (PID=822) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:31:18.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:31:18.676+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:31:18.675+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:31:18.686+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:31:18.684+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:31:18.687+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:31:18.726+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T15:31:48.872+0000] {processor.py:161} INFO - Started process (PID=844) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:31:48.874+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:31:48.878+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:31:48.877+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:31:48.884+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:31:48.882+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:31:48.886+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:31:48.915+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.047 seconds
[2024-03-23T15:32:19.088+0000] {processor.py:161} INFO - Started process (PID=860) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:32:19.090+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:32:19.095+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:32:19.093+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:32:19.107+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:32:19.102+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:32:19.109+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:32:19.142+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T15:32:49.358+0000] {processor.py:161} INFO - Started process (PID=876) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:32:49.359+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:32:49.364+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:32:49.363+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:32:49.373+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:32:49.371+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:32:49.375+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:32:49.412+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T15:33:19.614+0000] {processor.py:161} INFO - Started process (PID=892) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:33:19.616+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:33:19.624+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:33:19.622+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:33:19.638+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:33:19.634+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:33:19.641+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:33:19.678+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.074 seconds
[2024-03-23T15:33:49.889+0000] {processor.py:161} INFO - Started process (PID=908) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:33:49.892+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:33:49.898+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:33:49.897+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:33:49.905+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:33:49.903+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:33:49.906+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:33:49.942+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T15:34:20.126+0000] {processor.py:161} INFO - Started process (PID=924) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:34:20.128+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:34:20.136+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:34:20.135+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:34:20.148+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:34:20.145+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:34:20.150+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:34:20.187+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T15:34:50.406+0000] {processor.py:161} INFO - Started process (PID=940) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:34:50.408+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:34:50.416+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:34:50.414+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:34:50.429+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:34:50.425+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:34:50.431+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:34:50.464+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T15:35:20.675+0000] {processor.py:161} INFO - Started process (PID=956) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:35:20.677+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:35:20.683+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:35:20.682+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:35:20.694+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:35:20.692+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:35:20.695+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:35:20.725+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T15:35:50.934+0000] {processor.py:161} INFO - Started process (PID=972) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:35:50.936+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:35:50.941+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:35:50.940+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:35:50.957+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:35:50.950+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:35:50.959+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:35:51.011+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.084 seconds
[2024-03-23T15:36:21.236+0000] {processor.py:161} INFO - Started process (PID=988) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:36:21.239+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:36:21.246+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:36:21.245+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:36:21.253+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:36:21.251+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:36:21.254+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:36:21.296+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T15:36:51.488+0000] {processor.py:161} INFO - Started process (PID=1004) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:36:51.490+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:36:51.495+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:36:51.494+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:36:51.508+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:36:51.505+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:36:51.511+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:36:51.550+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T15:37:21.750+0000] {processor.py:161} INFO - Started process (PID=1020) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:37:21.752+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:37:21.757+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:37:21.756+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:37:21.771+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:37:21.767+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:37:21.773+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:37:21.811+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T15:37:52.030+0000] {processor.py:161} INFO - Started process (PID=1036) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:37:52.033+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:37:52.037+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:37:52.036+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:37:52.046+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:37:52.044+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:37:52.047+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:37:52.074+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.054 seconds
[2024-03-23T15:38:22.255+0000] {processor.py:161} INFO - Started process (PID=1052) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:38:22.258+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:38:22.263+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:38:22.263+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:38:22.273+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:38:22.271+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:38:22.275+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:38:22.315+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T15:38:52.528+0000] {processor.py:161} INFO - Started process (PID=1068) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:38:52.530+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:38:52.534+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:38:52.533+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:38:52.547+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:38:52.545+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:38:52.548+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:38:52.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.071 seconds
[2024-03-23T15:39:22.812+0000] {processor.py:161} INFO - Started process (PID=1085) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:39:22.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:39:22.820+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:39:22.819+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:39:22.835+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:39:22.831+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:39:22.838+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:39:22.865+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T15:39:53.043+0000] {processor.py:161} INFO - Started process (PID=1101) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:39:53.044+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:39:53.050+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:39:53.048+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:39:53.065+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:39:53.060+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:39:53.067+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:39:53.105+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T15:40:23.306+0000] {processor.py:161} INFO - Started process (PID=1117) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:40:23.307+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:40:23.311+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:40:23.310+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:40:23.320+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:40:23.317+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:40:23.321+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:40:23.365+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T15:40:53.614+0000] {processor.py:161} INFO - Started process (PID=1133) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:40:53.616+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:40:53.623+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:40:53.622+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:40:53.632+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:40:53.629+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:40:53.633+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:40:53.661+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.054 seconds
[2024-03-23T15:41:23.878+0000] {processor.py:161} INFO - Started process (PID=1149) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:41:23.880+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:41:23.887+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:41:23.886+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:41:23.899+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:41:23.897+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:41:23.901+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:41:23.944+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.069 seconds
[2024-03-23T15:41:54.154+0000] {processor.py:161} INFO - Started process (PID=1165) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:41:54.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:41:54.158+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:41:54.158+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:41:54.171+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:41:54.167+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:41:54.173+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:41:54.212+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T15:42:24.417+0000] {processor.py:161} INFO - Started process (PID=1181) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:42:24.419+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:42:24.426+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:42:24.424+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:42:24.438+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:42:24.435+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:42:24.439+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:42:24.474+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T15:42:54.685+0000] {processor.py:161} INFO - Started process (PID=1197) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:42:54.689+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:42:54.697+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:42:54.695+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:42:54.707+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:42:54.703+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:42:54.709+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:42:54.746+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T15:43:24.950+0000] {processor.py:161} INFO - Started process (PID=1213) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:43:24.952+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:43:24.958+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:43:24.956+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:43:24.970+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:43:24.967+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:43:24.974+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:43:25.017+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.072 seconds
[2024-03-23T15:43:55.214+0000] {processor.py:161} INFO - Started process (PID=1229) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:43:55.216+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:43:55.219+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:43:55.219+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:43:55.226+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:43:55.224+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:43:55.227+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:43:55.261+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.052 seconds
[2024-03-23T15:44:25.414+0000] {processor.py:161} INFO - Started process (PID=1245) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:44:25.416+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:44:25.421+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:44:25.421+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:44:25.434+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:44:25.431+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:44:25.436+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:44:25.469+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T15:44:55.680+0000] {processor.py:161} INFO - Started process (PID=1261) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:44:55.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:44:55.690+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:44:55.689+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:44:55.701+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:44:55.699+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:44:55.702+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:44:55.734+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T15:45:25.941+0000] {processor.py:161} INFO - Started process (PID=1277) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:45:25.943+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:45:25.950+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:45:25.949+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:45:25.963+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:45:25.960+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:45:25.965+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:45:26.003+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.072 seconds
[2024-03-23T15:45:56.225+0000] {processor.py:161} INFO - Started process (PID=1293) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:45:56.226+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:45:56.231+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:45:56.230+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:45:56.244+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:45:56.239+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:45:56.245+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:45:56.280+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T15:46:26.501+0000] {processor.py:161} INFO - Started process (PID=1309) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:46:26.504+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:46:26.509+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:46:26.508+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:46:26.519+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:46:26.515+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:46:26.521+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:46:26.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.080 seconds
[2024-03-23T15:46:56.768+0000] {processor.py:161} INFO - Started process (PID=1325) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:46:56.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:46:56.774+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:46:56.773+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:46:56.783+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:46:56.781+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:46:56.785+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:46:56.832+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T15:47:27.039+0000] {processor.py:161} INFO - Started process (PID=1341) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:47:27.042+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:47:27.048+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:47:27.046+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:47:27.059+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:47:27.056+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:47:27.062+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:47:27.107+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.076 seconds
[2024-03-23T15:47:57.315+0000] {processor.py:161} INFO - Started process (PID=1357) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:47:57.319+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:47:57.329+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:47:57.326+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:47:57.352+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:47:57.349+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:47:57.354+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:47:57.401+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.097 seconds
[2024-03-23T15:48:27.614+0000] {processor.py:161} INFO - Started process (PID=1373) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:48:27.616+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:48:27.620+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:48:27.619+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:48:27.627+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:48:27.625+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:48:27.628+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:48:27.654+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.045 seconds
[2024-03-23T15:48:57.843+0000] {processor.py:161} INFO - Started process (PID=1389) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:48:57.846+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:48:57.851+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:48:57.851+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:48:57.863+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:48:57.860+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:48:57.864+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:48:57.906+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.073 seconds
[2024-03-23T15:49:28.106+0000] {processor.py:161} INFO - Started process (PID=1405) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:49:28.107+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:49:28.110+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:49:28.110+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:49:28.123+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:49:28.120+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:49:28.124+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:49:28.160+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T15:49:58.321+0000] {processor.py:161} INFO - Started process (PID=1421) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:49:58.323+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:49:58.327+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:49:58.327+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:49:58.336+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:49:58.334+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:49:58.337+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:49:58.367+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.054 seconds
[2024-03-23T15:50:28.529+0000] {processor.py:161} INFO - Started process (PID=1437) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:50:28.530+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:50:28.533+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:50:28.533+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:50:28.546+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:50:28.542+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:50:28.547+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:50:28.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T15:50:58.833+0000] {processor.py:161} INFO - Started process (PID=1453) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:50:58.834+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:50:58.838+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:50:58.837+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:50:58.850+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:50:58.845+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:50:58.852+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:50:58.889+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T15:51:29.054+0000] {processor.py:161} INFO - Started process (PID=1469) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:51:29.056+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:51:29.059+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:51:29.058+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:51:29.070+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:51:29.067+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:51:29.072+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:51:29.104+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T15:51:59.338+0000] {processor.py:161} INFO - Started process (PID=1485) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:51:59.340+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:51:59.347+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:51:59.346+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:51:59.356+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:51:59.353+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:51:59.357+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:51:59.392+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T15:52:29.594+0000] {processor.py:161} INFO - Started process (PID=1501) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:52:29.596+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:52:29.600+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:52:29.599+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:52:29.618+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:52:29.615+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:52:29.620+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:52:29.647+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.057 seconds
[2024-03-23T15:52:59.820+0000] {processor.py:161} INFO - Started process (PID=1517) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:52:59.821+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:52:59.826+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:52:59.825+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:52:59.838+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:52:59.836+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:52:59.839+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:52:59.876+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T15:53:30.098+0000] {processor.py:161} INFO - Started process (PID=1533) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:53:30.101+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:53:30.107+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:53:30.105+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:53:30.118+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:53:30.115+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:53:30.119+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:53:30.142+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.052 seconds
[2024-03-23T15:54:00.287+0000] {processor.py:161} INFO - Started process (PID=1549) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:54:00.289+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:54:00.294+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:54:00.293+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:54:00.303+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:54:00.300+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:54:00.305+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:54:00.342+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T15:54:30.521+0000] {processor.py:161} INFO - Started process (PID=1565) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:54:30.523+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:54:30.527+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:54:30.526+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:54:30.535+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:54:30.533+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:54:30.536+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:54:30.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.056 seconds
[2024-03-23T15:55:00.770+0000] {processor.py:161} INFO - Started process (PID=1581) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:55:00.772+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:55:00.779+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:55:00.777+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:55:00.803+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:55:00.796+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:55:00.805+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:55:00.854+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.091 seconds
[2024-03-23T15:55:31.048+0000] {processor.py:161} INFO - Started process (PID=1597) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:55:31.050+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:55:31.054+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:55:31.054+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:55:31.063+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:55:31.060+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:55:31.065+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:55:31.132+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.090 seconds
[2024-03-23T15:56:01.283+0000] {processor.py:161} INFO - Started process (PID=1613) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:56:01.284+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:56:01.288+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:56:01.287+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:56:01.297+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:56:01.295+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:56:01.298+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:56:01.334+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.055 seconds
[2024-03-23T15:56:31.520+0000] {processor.py:161} INFO - Started process (PID=1629) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:56:31.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:56:31.525+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:56:31.524+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:56:31.533+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:56:31.531+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:56:31.534+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:56:31.576+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T15:57:01.780+0000] {processor.py:161} INFO - Started process (PID=1644) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:57:01.782+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:57:01.787+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:57:01.786+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:57:01.796+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:57:01.794+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:57:01.798+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:57:01.836+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T15:57:32.007+0000] {processor.py:161} INFO - Started process (PID=1660) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:57:32.008+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:57:32.012+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:57:32.011+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:57:32.021+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:57:32.019+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:57:32.022+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:57:32.054+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.052 seconds
[2024-03-23T15:58:02.201+0000] {processor.py:161} INFO - Started process (PID=1676) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:58:02.203+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:58:02.207+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:58:02.206+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:58:02.215+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:58:02.211+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:58:02.218+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:58:02.261+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T15:58:32.450+0000] {processor.py:161} INFO - Started process (PID=1692) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:58:32.452+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:58:32.456+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:58:32.455+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:58:32.465+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:58:32.462+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:58:32.466+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:58:32.499+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.053 seconds
[2024-03-23T15:59:02.646+0000] {processor.py:161} INFO - Started process (PID=1708) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:59:02.654+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:59:02.658+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:59:02.658+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:59:02.673+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:59:02.666+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:59:02.675+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:59:02.722+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.081 seconds
[2024-03-23T15:59:32.908+0000] {processor.py:161} INFO - Started process (PID=1724) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T15:59:32.910+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T15:59:32.915+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:59:32.914+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T15:59:32.923+0000] {logging_mixin.py:188} INFO - [2024-03-23T15:59:32.921+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T15:59:32.924+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T15:59:32.962+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T16:00:03.164+0000] {processor.py:161} INFO - Started process (PID=1740) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:00:03.166+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:00:03.171+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:00:03.170+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:00:03.182+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:00:03.179+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:00:03.183+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:00:03.227+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T16:00:33.430+0000] {processor.py:161} INFO - Started process (PID=1756) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:00:33.432+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:00:33.438+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:00:33.437+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:00:33.454+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:00:33.451+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:00:33.457+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:00:33.503+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.078 seconds
[2024-03-23T16:01:03.715+0000] {processor.py:161} INFO - Started process (PID=1772) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:01:03.717+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:01:03.724+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:01:03.722+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:01:03.732+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:01:03.731+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:01:03.733+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:01:03.766+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T16:01:33.975+0000] {processor.py:161} INFO - Started process (PID=1788) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:01:33.978+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:01:33.984+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:01:33.983+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:01:33.993+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:01:33.991+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:01:33.994+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:01:34.029+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T16:02:04.242+0000] {processor.py:161} INFO - Started process (PID=1804) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:02:04.245+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:02:04.248+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:02:04.248+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:02:04.257+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:02:04.254+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:02:04.259+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:02:04.296+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T16:02:34.482+0000] {processor.py:161} INFO - Started process (PID=1820) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:02:34.484+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:02:34.489+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:02:34.488+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:02:34.501+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:02:34.497+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:02:34.502+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:02:34.537+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T16:03:04.752+0000] {processor.py:161} INFO - Started process (PID=1836) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:03:04.754+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:03:04.759+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:03:04.758+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:03:04.769+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:03:04.767+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:03:04.771+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:03:04.814+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T16:03:35.035+0000] {processor.py:161} INFO - Started process (PID=1851) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:03:35.036+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:03:35.042+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:03:35.041+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:03:35.053+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:03:35.049+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:03:35.055+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:03:35.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T16:04:05.310+0000] {processor.py:161} INFO - Started process (PID=1867) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:04:05.313+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:04:05.320+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:04:05.318+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:04:05.332+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:04:05.330+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:04:05.333+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:04:05.367+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T16:04:35.576+0000] {processor.py:161} INFO - Started process (PID=1883) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:04:35.578+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:04:35.582+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:04:35.581+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:04:35.594+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:04:35.590+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:04:35.596+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:04:35.639+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T16:05:05.780+0000] {processor.py:161} INFO - Started process (PID=1899) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:05:05.782+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:05:05.786+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:05:05.785+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:05:05.796+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:05:05.794+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:05:05.798+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:05:05.836+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T16:05:36.046+0000] {processor.py:161} INFO - Started process (PID=1915) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:05:36.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:05:36.055+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:05:36.054+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:05:36.066+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:05:36.063+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:05:36.067+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:05:36.104+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T16:06:06.340+0000] {processor.py:161} INFO - Started process (PID=1931) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:06:06.342+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:06:06.349+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:06:06.348+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:06:06.363+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:06:06.359+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:06:06.364+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:06:06.400+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T16:06:36.550+0000] {processor.py:161} INFO - Started process (PID=1947) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:06:36.552+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:06:36.561+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:06:36.559+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:06:36.575+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:06:36.570+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:06:36.578+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:06:36.621+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.078 seconds
[2024-03-23T16:07:06.845+0000] {processor.py:161} INFO - Started process (PID=1963) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:07:06.848+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:07:06.855+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:07:06.853+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:07:06.864+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:07:06.861+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:07:06.867+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:07:06.904+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T16:07:37.098+0000] {processor.py:161} INFO - Started process (PID=1979) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:07:37.101+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:07:37.105+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:07:37.104+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:07:37.112+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:07:37.110+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:07:37.114+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:07:37.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T16:08:07.355+0000] {processor.py:161} INFO - Started process (PID=1995) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:08:07.357+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:08:07.360+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:08:07.360+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:08:07.369+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:08:07.367+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:08:07.370+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:08:07.400+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.054 seconds
[2024-03-23T16:08:37.591+0000] {processor.py:161} INFO - Started process (PID=2011) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:08:37.593+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:08:37.597+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:08:37.596+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:08:37.609+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:08:37.606+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:08:37.612+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:08:37.655+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T16:09:07.863+0000] {processor.py:161} INFO - Started process (PID=2027) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:09:07.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:09:07.872+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:09:07.870+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:09:07.885+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:09:07.881+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:09:07.887+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:09:07.921+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T16:09:38.127+0000] {processor.py:161} INFO - Started process (PID=2043) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:09:38.129+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:09:38.135+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:09:38.134+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:09:38.144+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:09:38.141+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:09:38.145+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:09:38.180+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T16:10:08.397+0000] {processor.py:161} INFO - Started process (PID=2059) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:10:08.400+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:10:08.406+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:10:08.405+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:10:08.416+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:10:08.414+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:10:08.417+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:10:08.465+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.078 seconds
[2024-03-23T16:10:38.688+0000] {processor.py:161} INFO - Started process (PID=2075) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:10:38.690+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:10:38.697+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:10:38.696+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:10:38.709+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:10:38.706+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:10:38.711+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:10:38.747+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T16:11:08.955+0000] {processor.py:161} INFO - Started process (PID=2091) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:11:08.957+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:11:08.964+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:11:08.963+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:11:08.972+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:11:08.970+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:11:08.973+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:11:09.009+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T16:11:39.180+0000] {processor.py:161} INFO - Started process (PID=2107) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:11:39.182+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:11:39.185+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:11:39.185+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:11:39.196+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:11:39.194+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:11:39.197+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:11:39.234+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T16:12:09.448+0000] {processor.py:161} INFO - Started process (PID=2123) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:12:09.451+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:12:09.458+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:12:09.456+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:12:09.470+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:12:09.465+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:12:09.472+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:12:09.511+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.071 seconds
[2024-03-23T16:12:39.715+0000] {processor.py:161} INFO - Started process (PID=2139) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:12:39.717+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:12:39.722+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:12:39.721+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:12:39.733+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:12:39.729+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:12:39.735+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:12:39.765+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T16:13:09.990+0000] {processor.py:161} INFO - Started process (PID=2155) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:13:09.992+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:13:09.998+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:13:09.997+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:13:10.013+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:13:10.008+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:13:10.015+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:13:10.054+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.073 seconds
[2024-03-23T16:13:40.238+0000] {processor.py:161} INFO - Started process (PID=2171) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:13:40.240+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:13:40.248+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:13:40.246+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:13:40.258+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:13:40.255+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:13:40.260+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:13:40.285+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.050 seconds
[2024-03-23T16:14:10.503+0000] {processor.py:161} INFO - Started process (PID=2187) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:14:10.506+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:14:10.519+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:14:10.516+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:14:10.534+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:14:10.531+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:14:10.538+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:14:10.594+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.098 seconds
[2024-03-23T16:14:40.795+0000] {processor.py:161} INFO - Started process (PID=2203) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:14:40.797+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:14:40.803+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:14:40.802+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:14:40.816+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:14:40.812+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:14:40.817+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:14:40.852+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T16:15:11.072+0000] {processor.py:161} INFO - Started process (PID=2219) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:15:11.075+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:15:11.083+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:15:11.082+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:15:11.093+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:15:11.090+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:15:11.095+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:15:11.129+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T16:15:41.343+0000] {processor.py:161} INFO - Started process (PID=2235) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:15:41.344+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:15:41.348+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:15:41.347+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:15:41.356+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:15:41.354+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:15:41.357+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:15:41.384+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.045 seconds
[2024-03-23T16:16:11.591+0000] {processor.py:161} INFO - Started process (PID=2251) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:16:11.593+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:16:11.599+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:16:11.598+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:16:11.608+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:16:11.606+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:16:11.610+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:16:11.646+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T16:16:41.856+0000] {processor.py:161} INFO - Started process (PID=2267) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:16:41.859+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:16:41.865+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:16:41.864+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:16:41.875+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:16:41.871+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:16:41.877+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:16:41.914+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T16:17:12.115+0000] {processor.py:161} INFO - Started process (PID=2283) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:17:12.117+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:17:12.122+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:17:12.121+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:17:12.133+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:17:12.130+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:17:12.135+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:17:12.187+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.079 seconds
[2024-03-23T16:17:42.393+0000] {processor.py:161} INFO - Started process (PID=2299) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:17:42.395+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:17:42.401+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:17:42.400+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:17:42.412+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:17:42.409+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:17:42.414+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:17:42.462+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.075 seconds
[2024-03-23T16:18:12.660+0000] {processor.py:161} INFO - Started process (PID=2315) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:18:12.663+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:18:12.668+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:18:12.667+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:18:12.678+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:18:12.675+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:18:12.679+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:18:12.706+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.055 seconds
[2024-03-23T16:18:42.907+0000] {processor.py:161} INFO - Started process (PID=2331) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:18:42.909+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:18:42.913+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:18:42.913+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:18:42.925+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:18:42.921+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:18:42.928+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:18:42.973+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.071 seconds
[2024-03-23T16:19:13.186+0000] {processor.py:161} INFO - Started process (PID=2347) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:19:13.189+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:19:13.196+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:19:13.195+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:19:13.205+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:19:13.203+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:19:13.206+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:19:13.238+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T16:19:43.405+0000] {processor.py:161} INFO - Started process (PID=2363) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:19:43.408+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:19:43.414+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:19:43.413+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:19:43.426+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:19:43.423+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:19:43.428+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:19:43.463+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T16:20:13.618+0000] {processor.py:161} INFO - Started process (PID=2379) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:20:13.620+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:20:13.623+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:20:13.622+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:20:13.635+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:20:13.630+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:20:13.638+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:20:13.665+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.055 seconds
[2024-03-23T16:20:43.920+0000] {processor.py:161} INFO - Started process (PID=2394) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:20:43.922+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:20:43.929+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:20:43.928+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:20:43.940+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:20:43.937+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:20:43.943+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:20:43.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.053 seconds
[2024-03-23T16:21:14.151+0000] {processor.py:161} INFO - Started process (PID=2410) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:21:14.153+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:21:14.157+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:21:14.157+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:21:14.173+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:21:14.169+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:21:14.175+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:21:14.214+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T16:21:44.398+0000] {processor.py:161} INFO - Started process (PID=2426) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:21:44.400+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:21:44.403+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:21:44.402+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:21:44.416+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:21:44.412+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:21:44.418+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:21:44.455+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T16:22:14.631+0000] {processor.py:161} INFO - Started process (PID=2442) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:22:14.632+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:22:14.636+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:22:14.635+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:22:14.644+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:22:14.642+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:22:14.646+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:22:14.689+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T16:22:44.893+0000] {processor.py:161} INFO - Started process (PID=2457) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:22:44.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:22:44.903+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:22:44.902+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:22:44.917+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:22:44.915+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:22:44.919+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:22:44.946+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T16:23:15.164+0000] {processor.py:161} INFO - Started process (PID=2473) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:23:15.166+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:23:15.172+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:23:15.171+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:23:15.184+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:23:15.181+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:23:15.186+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:23:15.227+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T16:23:45.444+0000] {processor.py:161} INFO - Started process (PID=2490) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:23:45.445+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:23:45.452+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:23:45.450+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:23:45.463+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:23:45.461+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:23:45.465+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:23:45.506+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.069 seconds
[2024-03-23T16:24:15.715+0000] {processor.py:161} INFO - Started process (PID=2506) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:24:15.718+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:24:15.725+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:24:15.723+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:24:15.740+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:24:15.735+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:24:15.742+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:24:15.776+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.069 seconds
[2024-03-23T16:24:45.986+0000] {processor.py:161} INFO - Started process (PID=2522) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:24:45.988+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:24:45.996+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:24:45.994+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:24:46.010+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:24:46.008+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:24:46.011+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:24:46.036+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T16:25:16.260+0000] {processor.py:161} INFO - Started process (PID=2538) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:25:16.262+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:25:16.269+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:25:16.267+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:25:16.282+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:25:16.279+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:25:16.283+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:25:16.317+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T16:25:46.550+0000] {processor.py:161} INFO - Started process (PID=2554) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:25:46.552+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:25:46.558+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:25:46.557+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:25:46.571+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:25:46.567+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:25:46.572+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:25:46.602+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T16:26:16.802+0000] {processor.py:161} INFO - Started process (PID=2570) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:26:16.804+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:26:16.810+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:26:16.809+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:26:16.820+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:26:16.817+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:26:16.822+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:26:16.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T16:26:47.015+0000] {processor.py:161} INFO - Started process (PID=2586) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:26:47.016+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:26:47.020+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:26:47.020+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:26:47.029+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:26:47.027+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:26:47.030+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:26:47.066+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.055 seconds
[2024-03-23T16:27:17.271+0000] {processor.py:161} INFO - Started process (PID=2602) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:27:17.272+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:27:17.276+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:27:17.276+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:27:17.287+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:27:17.284+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:27:17.289+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:27:17.331+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T16:27:47.537+0000] {processor.py:161} INFO - Started process (PID=2618) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:27:47.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:27:47.544+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:27:47.543+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:27:47.554+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:27:47.550+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:27:47.556+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:27:47.597+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T16:28:17.805+0000] {processor.py:161} INFO - Started process (PID=2634) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:28:17.807+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:28:17.813+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:28:17.812+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:28:17.822+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:28:17.819+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:28:17.824+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:28:17.863+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T16:28:48.071+0000] {processor.py:161} INFO - Started process (PID=2650) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:28:48.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:28:48.085+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:28:48.083+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:28:48.100+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:28:48.096+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:28:48.102+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:28:48.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.084 seconds
[2024-03-23T16:29:18.395+0000] {processor.py:161} INFO - Started process (PID=2666) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:29:18.398+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:29:18.403+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:29:18.402+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:29:18.414+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:29:18.411+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:29:18.416+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:29:18.445+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T16:29:48.647+0000] {processor.py:161} INFO - Started process (PID=2682) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:29:48.649+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:29:48.653+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:29:48.652+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:29:48.664+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:29:48.661+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:29:48.666+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:29:48.708+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T16:30:18.908+0000] {processor.py:161} INFO - Started process (PID=2698) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:30:18.910+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:30:18.914+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:30:18.913+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:30:18.923+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:30:18.921+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:30:18.924+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:30:18.959+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.057 seconds
[2024-03-23T16:30:49.192+0000] {processor.py:161} INFO - Started process (PID=2714) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:30:49.195+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:30:49.202+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:30:49.201+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:30:49.210+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:30:49.208+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:30:49.211+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:30:49.244+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T16:31:19.450+0000] {processor.py:161} INFO - Started process (PID=2730) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:31:19.453+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:31:19.459+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:31:19.458+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:31:19.473+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:31:19.468+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:31:19.476+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:31:19.501+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T16:31:49.710+0000] {processor.py:161} INFO - Started process (PID=2746) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:31:49.713+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:31:49.720+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:31:49.718+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:31:49.731+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:31:49.729+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:31:49.732+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:31:49.762+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T16:32:19.972+0000] {processor.py:161} INFO - Started process (PID=2762) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:32:19.975+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:32:19.981+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:32:19.980+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:32:19.990+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:32:19.988+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:32:19.991+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:32:20.038+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.074 seconds
[2024-03-23T16:32:50.239+0000] {processor.py:161} INFO - Started process (PID=2778) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:32:50.242+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:32:50.249+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:32:50.248+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:32:50.257+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:32:50.255+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:32:50.259+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:32:50.303+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.071 seconds
[2024-03-23T16:33:20.517+0000] {processor.py:161} INFO - Started process (PID=2794) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:33:20.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:33:20.526+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:33:20.525+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:33:20.538+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:33:20.533+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:33:20.539+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:33:20.564+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.055 seconds
[2024-03-23T16:33:50.778+0000] {processor.py:161} INFO - Started process (PID=2810) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:33:50.781+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:33:50.787+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:33:50.786+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:33:50.798+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:33:50.794+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:33:50.800+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:33:50.846+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.075 seconds
[2024-03-23T16:34:21.049+0000] {processor.py:161} INFO - Started process (PID=2826) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:34:21.050+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:34:21.059+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:34:21.057+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:34:21.073+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:34:21.069+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:34:21.076+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:34:21.110+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T16:34:51.300+0000] {processor.py:161} INFO - Started process (PID=2842) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:34:51.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:34:51.307+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:34:51.306+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:34:51.315+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:34:51.313+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:34:51.316+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:34:51.360+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T16:35:21.557+0000] {processor.py:161} INFO - Started process (PID=2858) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:35:21.558+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:35:21.561+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:35:21.561+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:35:21.569+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:35:21.567+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:35:21.570+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:35:21.609+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.056 seconds
[2024-03-23T16:35:51.858+0000] {processor.py:161} INFO - Started process (PID=2874) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:35:51.862+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:35:51.868+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:35:51.867+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:35:51.882+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:35:51.877+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:35:51.884+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:35:51.918+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.069 seconds
[2024-03-23T16:36:22.121+0000] {processor.py:161} INFO - Started process (PID=2890) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:36:22.123+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:36:22.128+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:36:22.127+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:36:22.139+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:36:22.136+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:36:22.141+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:36:22.186+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T16:36:52.395+0000] {processor.py:161} INFO - Started process (PID=2906) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:36:52.398+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:36:52.404+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:36:52.403+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:36:52.416+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:36:52.412+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:36:52.418+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:36:52.450+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T16:37:22.677+0000] {processor.py:161} INFO - Started process (PID=2922) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:37:22.679+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:37:22.685+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:37:22.684+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:37:22.697+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:37:22.694+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:37:22.699+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:37:22.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T16:37:52.939+0000] {processor.py:161} INFO - Started process (PID=2938) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:37:52.941+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:37:52.947+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:37:52.946+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:37:52.961+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:37:52.958+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:37:52.962+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:37:52.992+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T16:38:23.242+0000] {processor.py:161} INFO - Started process (PID=2954) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:38:23.244+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:38:23.251+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:38:23.248+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:38:23.263+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:38:23.259+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:38:23.265+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:38:23.309+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.079 seconds
[2024-03-23T16:38:53.527+0000] {processor.py:161} INFO - Started process (PID=2970) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:38:53.529+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:38:53.534+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:38:53.533+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:38:53.546+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:38:53.543+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:38:53.549+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:38:53.587+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T16:39:23.806+0000] {processor.py:161} INFO - Started process (PID=2986) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:39:23.810+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:39:23.816+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:39:23.815+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:39:23.830+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:39:23.826+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:39:23.832+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:39:23.881+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.085 seconds
[2024-03-23T16:39:54.072+0000] {processor.py:161} INFO - Started process (PID=3002) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:39:54.074+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:39:54.077+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:39:54.077+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:39:54.085+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:39:54.083+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:39:54.087+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:39:54.120+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.052 seconds
[2024-03-23T16:40:24.319+0000] {processor.py:161} INFO - Started process (PID=3018) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:40:24.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:40:24.326+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:40:24.325+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:40:24.335+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:40:24.332+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:40:24.336+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:40:24.363+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.049 seconds
[2024-03-23T16:40:54.567+0000] {processor.py:161} INFO - Started process (PID=3034) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:40:54.570+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:40:54.576+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:40:54.575+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:40:54.588+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:40:54.586+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:40:54.590+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:40:54.622+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T16:41:24.850+0000] {processor.py:161} INFO - Started process (PID=3050) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:41:24.852+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:41:24.858+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:41:24.857+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:41:24.870+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:41:24.866+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:41:24.872+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:41:24.921+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.081 seconds
[2024-03-23T16:41:55.107+0000] {processor.py:161} INFO - Started process (PID=3066) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:41:55.108+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:41:55.112+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:41:55.111+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:41:55.120+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:41:55.117+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:41:55.123+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:41:55.160+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.056 seconds
[2024-03-23T16:42:25.340+0000] {processor.py:161} INFO - Started process (PID=3082) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:42:25.343+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:42:25.349+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:42:25.348+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:42:25.361+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:42:25.358+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:42:25.362+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:42:25.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T16:42:55.557+0000] {processor.py:161} INFO - Started process (PID=3098) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:42:55.560+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:42:55.567+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:42:55.565+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:42:55.577+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:42:55.575+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:42:55.578+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:42:55.612+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T16:43:25.773+0000] {processor.py:161} INFO - Started process (PID=3114) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:43:25.775+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:43:25.781+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:43:25.780+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:43:25.792+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:43:25.789+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:43:25.793+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:43:25.826+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T16:43:56.029+0000] {processor.py:161} INFO - Started process (PID=3130) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:43:56.031+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:43:56.037+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:43:56.036+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:43:56.045+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:43:56.043+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:43:56.046+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:43:56.070+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.048 seconds
[2024-03-23T16:44:26.260+0000] {processor.py:161} INFO - Started process (PID=3146) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:44:26.262+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:44:26.265+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:44:26.264+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:44:26.272+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:44:26.270+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:44:26.273+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:44:26.297+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.045 seconds
[2024-03-23T16:44:56.477+0000] {processor.py:161} INFO - Started process (PID=3162) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:44:56.480+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:44:56.486+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:44:56.485+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:44:56.494+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:44:56.491+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:44:56.495+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:44:56.530+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T16:45:26.738+0000] {processor.py:161} INFO - Started process (PID=3178) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:45:26.741+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:45:26.748+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:45:26.746+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:45:26.758+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:45:26.755+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:45:26.760+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:45:26.796+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T16:45:57.019+0000] {processor.py:161} INFO - Started process (PID=3194) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:45:57.021+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:45:57.024+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:45:57.023+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:45:57.032+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:45:57.030+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:45:57.033+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:45:57.074+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T16:46:27.293+0000] {processor.py:161} INFO - Started process (PID=3210) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:46:27.295+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:46:27.300+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:46:27.299+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:46:27.312+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:46:27.308+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:46:27.313+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:46:27.358+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.072 seconds
[2024-03-23T16:46:57.571+0000] {processor.py:161} INFO - Started process (PID=3226) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:46:57.573+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:46:57.578+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:46:57.577+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:46:57.594+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:46:57.589+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:46:57.595+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:46:57.638+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.073 seconds
[2024-03-23T16:47:27.835+0000] {processor.py:161} INFO - Started process (PID=3242) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:47:27.837+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:47:27.844+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:47:27.843+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:47:27.852+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:47:27.850+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:47:27.853+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:47:27.893+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T16:47:58.030+0000] {processor.py:161} INFO - Started process (PID=3258) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:47:58.031+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:47:58.037+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:47:58.036+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:47:58.049+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:47:58.046+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:47:58.050+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:47:58.080+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.055 seconds
[2024-03-23T16:48:28.231+0000] {processor.py:161} INFO - Started process (PID=3274) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:48:28.233+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:48:28.237+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:48:28.236+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:48:28.243+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:48:28.241+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:48:28.244+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:48:28.273+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.048 seconds
[2024-03-23T16:48:58.418+0000] {processor.py:161} INFO - Started process (PID=3290) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:48:58.420+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:48:58.424+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:48:58.423+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:48:58.433+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:48:58.430+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:48:58.434+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:48:58.471+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.057 seconds
[2024-03-23T16:49:28.681+0000] {processor.py:161} INFO - Started process (PID=3306) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:49:28.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:49:28.690+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:49:28.689+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:49:28.702+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:49:28.700+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:49:28.703+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:49:28.739+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T16:49:58.935+0000] {processor.py:161} INFO - Started process (PID=3322) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:49:58.937+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:49:58.942+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:49:58.941+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:49:58.965+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:49:58.962+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:49:58.966+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:49:59.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.084 seconds
[2024-03-23T16:50:29.226+0000] {processor.py:161} INFO - Started process (PID=3339) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:50:29.229+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:50:29.234+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:50:29.233+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:50:29.245+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:50:29.243+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:50:29.246+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:50:29.280+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T16:50:59.518+0000] {processor.py:161} INFO - Started process (PID=3355) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:50:59.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:50:59.525+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:50:59.524+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:50:59.538+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:50:59.536+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:50:59.541+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:50:59.590+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.078 seconds
[2024-03-23T16:51:29.799+0000] {processor.py:161} INFO - Started process (PID=3370) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:51:29.801+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:51:29.807+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:51:29.806+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:51:29.820+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:51:29.817+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:51:29.822+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:51:29.869+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.078 seconds
[2024-03-23T16:52:00.044+0000] {processor.py:161} INFO - Started process (PID=3386) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:52:00.046+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:52:00.051+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:52:00.050+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:52:00.060+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:52:00.057+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:52:00.061+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:52:00.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.072 seconds
[2024-03-23T16:52:30.310+0000] {processor.py:161} INFO - Started process (PID=3402) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:52:30.311+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:52:30.314+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:52:30.314+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:52:30.321+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:52:30.319+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:52:30.323+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:52:30.359+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.052 seconds
[2024-03-23T16:53:00.545+0000] {processor.py:161} INFO - Started process (PID=3418) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:53:00.547+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:53:00.551+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:53:00.550+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:53:00.559+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:53:00.556+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:53:00.560+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:53:00.596+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.054 seconds
[2024-03-23T16:53:30.802+0000] {processor.py:161} INFO - Started process (PID=3434) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:53:30.805+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:53:30.811+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:53:30.810+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:53:30.823+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:53:30.820+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:53:30.824+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:53:30.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T16:54:01.056+0000] {processor.py:161} INFO - Started process (PID=3450) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:54:01.060+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:54:01.067+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:54:01.066+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:54:01.082+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:54:01.078+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:54:01.085+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:54:01.120+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.072 seconds
[2024-03-23T16:54:31.320+0000] {processor.py:161} INFO - Started process (PID=3466) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:54:31.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:54:31.327+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:54:31.326+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:54:31.338+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:54:31.335+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:54:31.339+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:54:31.380+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T16:55:01.593+0000] {processor.py:161} INFO - Started process (PID=3482) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:55:01.596+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:55:01.603+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:55:01.601+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:55:01.613+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:55:01.610+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:55:01.615+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:55:01.654+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T16:55:31.836+0000] {processor.py:161} INFO - Started process (PID=3504) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:55:31.838+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:55:31.844+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:55:31.842+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:55:31.856+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:55:31.853+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:55:31.858+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:55:31.899+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T16:56:02.108+0000] {processor.py:161} INFO - Started process (PID=3520) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:56:02.111+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:56:02.115+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:56:02.115+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:56:02.127+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:56:02.124+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:56:02.129+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:56:02.166+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T16:56:32.354+0000] {processor.py:161} INFO - Started process (PID=3536) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:56:32.356+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:56:32.360+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:56:32.359+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:56:32.369+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:56:32.366+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:56:32.370+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:56:32.408+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T16:57:02.623+0000] {processor.py:161} INFO - Started process (PID=3552) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:57:02.625+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:57:02.632+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:57:02.631+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:57:02.645+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:57:02.642+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:57:02.648+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:57:02.676+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T16:57:32.899+0000] {processor.py:161} INFO - Started process (PID=3568) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:57:32.901+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:57:32.905+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:57:32.904+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:57:32.915+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:57:32.913+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:57:32.917+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:57:32.946+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.051 seconds
[2024-03-23T16:58:03.109+0000] {processor.py:161} INFO - Started process (PID=3584) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:58:03.110+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:58:03.114+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:58:03.114+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:58:03.124+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:58:03.122+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:58:03.125+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:58:03.153+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.047 seconds
[2024-03-23T16:58:33.366+0000] {processor.py:161} INFO - Started process (PID=3600) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:58:33.368+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:58:33.372+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:58:33.371+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:58:33.382+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:58:33.380+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:58:33.384+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:58:33.422+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T16:59:03.610+0000] {processor.py:161} INFO - Started process (PID=3616) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:59:03.612+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:59:03.618+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:59:03.616+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:59:03.630+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:59:03.628+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:59:03.631+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:59:03.665+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T16:59:33.865+0000] {processor.py:161} INFO - Started process (PID=3632) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T16:59:33.867+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T16:59:33.872+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:59:33.871+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T16:59:33.882+0000] {logging_mixin.py:188} INFO - [2024-03-23T16:59:33.880+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T16:59:33.884+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T16:59:33.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.053 seconds
[2024-03-23T17:00:04.125+0000] {processor.py:161} INFO - Started process (PID=3648) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:00:04.127+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:00:04.132+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:00:04.131+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:00:04.143+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:00:04.140+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:00:04.144+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:00:04.180+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T17:00:34.377+0000] {processor.py:161} INFO - Started process (PID=3664) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:00:34.378+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:00:34.381+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:00:34.381+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:00:34.388+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:00:34.386+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:00:34.389+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:00:34.421+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.047 seconds
[2024-03-23T17:01:04.626+0000] {processor.py:161} INFO - Started process (PID=3680) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:01:04.629+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:01:04.635+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:01:04.634+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:01:04.643+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:01:04.641+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:01:04.644+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:01:04.676+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.056 seconds
[2024-03-23T17:01:34.879+0000] {processor.py:161} INFO - Started process (PID=3696) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:01:34.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:01:34.886+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:01:34.885+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:01:34.897+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:01:34.893+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:01:34.901+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:01:34.937+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T17:02:05.161+0000] {processor.py:161} INFO - Started process (PID=3712) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:02:05.164+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:02:05.171+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:02:05.170+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:02:05.183+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:02:05.181+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:02:05.184+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:02:05.219+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T17:02:35.431+0000] {processor.py:161} INFO - Started process (PID=3728) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:02:35.434+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:02:35.439+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:02:35.438+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:02:35.451+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:02:35.447+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:02:35.453+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:02:35.488+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T17:03:05.700+0000] {processor.py:161} INFO - Started process (PID=3744) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:03:05.702+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:03:05.705+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:03:05.704+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:03:05.713+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:03:05.711+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:03:05.714+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:03:05.742+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.045 seconds
[2024-03-23T17:03:35.905+0000] {processor.py:161} INFO - Started process (PID=3760) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:03:35.907+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:03:35.912+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:03:35.911+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:03:35.928+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:03:35.924+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:03:35.933+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:03:35.975+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.073 seconds
[2024-03-23T17:04:06.194+0000] {processor.py:161} INFO - Started process (PID=3776) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:04:06.197+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:04:06.202+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:04:06.201+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:04:06.209+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:04:06.207+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:04:06.211+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:04:06.244+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T17:04:36.482+0000] {processor.py:161} INFO - Started process (PID=3792) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:04:36.485+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:04:36.491+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:04:36.490+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:04:36.503+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:04:36.500+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:04:36.505+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:04:36.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.073 seconds
[2024-03-23T17:05:06.760+0000] {processor.py:161} INFO - Started process (PID=3808) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:05:06.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:05:06.769+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:05:06.767+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:05:06.782+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:05:06.780+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:05:06.784+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:05:06.808+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.056 seconds
[2024-03-23T17:05:37.009+0000] {processor.py:161} INFO - Started process (PID=3824) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:05:37.011+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:05:37.015+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:05:37.014+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:05:37.044+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:05:37.041+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:05:37.047+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:05:37.088+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.083 seconds
[2024-03-23T17:06:07.314+0000] {processor.py:161} INFO - Started process (PID=3840) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:06:07.316+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:06:07.323+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:06:07.321+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:06:07.334+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:06:07.331+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:06:07.336+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:06:07.375+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T17:06:37.523+0000] {processor.py:161} INFO - Started process (PID=3856) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:06:37.526+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:06:37.532+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:06:37.531+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:06:37.546+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:06:37.543+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:06:37.548+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:06:37.586+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T17:07:07.776+0000] {processor.py:161} INFO - Started process (PID=3872) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:07:07.778+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:07:07.784+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:07:07.783+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:07:07.798+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:07:07.794+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:07:07.800+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:07:07.845+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.074 seconds
[2024-03-23T17:07:38.048+0000] {processor.py:161} INFO - Started process (PID=3888) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:07:38.051+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:07:38.057+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:07:38.056+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:07:38.071+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:07:38.066+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:07:38.073+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:07:38.103+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T17:08:08.314+0000] {processor.py:161} INFO - Started process (PID=3904) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:08:08.316+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:08:08.320+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:08:08.319+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:08:08.332+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:08:08.328+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:08:08.334+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:08:08.375+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T17:08:38.594+0000] {processor.py:161} INFO - Started process (PID=3920) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:08:38.597+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:08:38.604+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:08:38.603+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:08:38.615+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:08:38.613+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:08:38.617+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:08:38.662+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.078 seconds
[2024-03-23T17:09:08.866+0000] {processor.py:161} INFO - Started process (PID=3936) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:09:08.869+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:09:08.874+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:09:08.874+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:09:08.888+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:09:08.885+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:09:08.890+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:09:08.930+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.072 seconds
[2024-03-23T17:09:39.131+0000] {processor.py:161} INFO - Started process (PID=3952) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:09:39.134+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:09:39.139+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:09:39.138+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:09:39.145+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:09:39.143+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:09:39.147+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:09:39.186+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T17:10:09.397+0000] {processor.py:161} INFO - Started process (PID=3968) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:10:09.399+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:10:09.404+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:10:09.403+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:10:09.418+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:10:09.413+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:10:09.420+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:10:09.454+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T17:10:39.623+0000] {processor.py:161} INFO - Started process (PID=3984) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:10:39.624+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:10:39.628+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:10:39.627+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:10:39.637+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:10:39.635+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:10:39.639+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:10:39.675+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.056 seconds
[2024-03-23T17:11:09.916+0000] {processor.py:161} INFO - Started process (PID=4000) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:11:09.918+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:11:09.925+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:11:09.924+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:11:09.933+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:11:09.931+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:11:09.934+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:11:09.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T17:11:40.183+0000] {processor.py:161} INFO - Started process (PID=4016) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:11:40.186+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:11:40.190+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:11:40.189+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:11:40.198+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:11:40.196+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:11:40.199+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:11:40.233+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.055 seconds
[2024-03-23T17:12:10.446+0000] {processor.py:161} INFO - Started process (PID=4032) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:12:10.448+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:12:10.454+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:12:10.453+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:12:10.462+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:12:10.460+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:12:10.464+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:12:10.489+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.052 seconds
[2024-03-23T17:12:40.700+0000] {processor.py:161} INFO - Started process (PID=4048) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:12:40.701+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:12:40.707+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:12:40.706+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:12:40.717+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:12:40.715+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:12:40.718+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:12:40.755+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T17:13:10.918+0000] {processor.py:161} INFO - Started process (PID=4064) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:13:10.919+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:13:10.924+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:13:10.923+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:13:10.936+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:13:10.932+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:13:10.938+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:13:10.978+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T17:13:41.208+0000] {processor.py:161} INFO - Started process (PID=4080) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:13:41.211+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:13:41.218+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:13:41.217+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:13:41.229+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:13:41.226+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:13:41.231+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:13:41.272+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.073 seconds
[2024-03-23T17:14:11.490+0000] {processor.py:161} INFO - Started process (PID=4096) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:14:11.493+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:14:11.498+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:14:11.498+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:14:11.509+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:14:11.507+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:14:11.511+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:14:11.555+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.072 seconds
[2024-03-23T17:14:41.772+0000] {processor.py:161} INFO - Started process (PID=4112) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:14:41.774+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:14:41.777+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:14:41.776+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:14:41.790+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:14:41.785+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:14:41.792+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:14:41.819+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.053 seconds
[2024-03-23T17:15:12.033+0000] {processor.py:161} INFO - Started process (PID=4128) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:15:12.036+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:15:12.043+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:15:12.041+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:15:12.054+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:15:12.052+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:15:12.056+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:15:12.086+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T17:15:42.313+0000] {processor.py:161} INFO - Started process (PID=4144) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:15:42.315+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:15:42.323+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:15:42.322+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:15:42.336+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:15:42.332+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:15:42.338+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:15:42.381+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.075 seconds
[2024-03-23T17:16:12.631+0000] {processor.py:161} INFO - Started process (PID=4160) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:16:12.634+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:16:12.641+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:16:12.639+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:16:12.653+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:16:12.650+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:16:12.654+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:16:12.689+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T17:16:42.880+0000] {processor.py:161} INFO - Started process (PID=4176) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:16:42.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:16:42.884+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:16:42.884+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:16:42.892+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:16:42.890+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:16:42.894+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:16:42.922+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.046 seconds
[2024-03-23T17:17:13.103+0000] {processor.py:161} INFO - Started process (PID=4192) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:17:13.106+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:17:13.113+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:17:13.111+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:17:13.124+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:17:13.121+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:17:13.126+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:17:13.162+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T17:17:43.373+0000] {processor.py:161} INFO - Started process (PID=4208) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:17:43.375+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:17:43.381+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:17:43.380+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:17:43.394+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:17:43.390+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:17:43.395+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:17:43.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T17:18:13.631+0000] {processor.py:161} INFO - Started process (PID=4224) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:18:13.633+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:18:13.639+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:18:13.638+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:18:13.649+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:18:13.648+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:18:13.650+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:18:13.685+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T17:18:43.887+0000] {processor.py:161} INFO - Started process (PID=4240) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:18:43.889+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:18:43.895+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:18:43.893+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:18:43.904+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:18:43.902+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:18:43.905+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:18:43.930+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.051 seconds
[2024-03-23T17:19:14.146+0000] {processor.py:161} INFO - Started process (PID=4256) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:19:14.149+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:19:14.154+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:19:14.153+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:19:14.168+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:19:14.165+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:19:14.170+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:19:14.210+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T17:19:44.424+0000] {processor.py:161} INFO - Started process (PID=4272) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:19:44.427+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:19:44.433+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:19:44.432+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:19:44.439+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:19:44.438+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:19:44.440+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:19:44.475+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T17:20:14.675+0000] {processor.py:161} INFO - Started process (PID=4288) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:20:14.677+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:20:14.683+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:20:14.682+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:20:14.696+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:20:14.691+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:20:14.699+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:20:14.731+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T17:20:44.959+0000] {processor.py:161} INFO - Started process (PID=4304) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:20:44.961+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:20:44.967+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:20:44.966+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:20:44.980+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:20:44.976+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:20:44.981+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:20:45.020+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.071 seconds
[2024-03-23T17:21:15.251+0000] {processor.py:161} INFO - Started process (PID=4320) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:21:15.253+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:21:15.261+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:21:15.259+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:21:15.269+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:21:15.267+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:21:15.271+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:21:15.310+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T17:21:45.534+0000] {processor.py:161} INFO - Started process (PID=4336) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:21:45.536+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:21:45.542+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:21:45.541+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:21:45.549+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:21:45.547+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:21:45.550+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:21:45.573+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.048 seconds
[2024-03-23T17:22:15.780+0000] {processor.py:161} INFO - Started process (PID=4352) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:22:15.782+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:22:15.790+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:22:15.789+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:22:15.803+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:22:15.799+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:22:15.805+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:22:15.851+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.076 seconds
[2024-03-23T17:22:46.010+0000] {processor.py:161} INFO - Started process (PID=4368) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:22:46.013+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:22:46.022+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:22:46.021+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:22:46.037+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:22:46.032+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:22:46.041+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:22:46.067+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T17:23:16.290+0000] {processor.py:161} INFO - Started process (PID=4384) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:23:16.293+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:23:16.300+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:23:16.299+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:23:16.306+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:23:16.304+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:23:16.307+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:23:16.341+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T17:23:46.554+0000] {processor.py:161} INFO - Started process (PID=4400) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:23:46.555+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:23:46.559+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:23:46.558+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:23:46.568+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:23:46.565+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:23:46.571+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:23:46.607+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T17:24:16.765+0000] {processor.py:161} INFO - Started process (PID=4416) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:24:16.767+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:24:16.771+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:24:16.770+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:24:16.787+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:24:16.783+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:24:16.788+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:24:16.832+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.071 seconds
[2024-03-23T17:24:47.050+0000] {processor.py:161} INFO - Started process (PID=4432) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:24:47.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:24:47.059+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:24:47.057+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:24:47.066+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:24:47.064+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:24:47.067+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:24:47.107+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T17:25:17.309+0000] {processor.py:161} INFO - Started process (PID=4448) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:25:17.312+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:25:17.317+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:25:17.315+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:25:17.328+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:25:17.325+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:25:17.330+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:25:17.369+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T17:25:47.553+0000] {processor.py:161} INFO - Started process (PID=4464) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:25:47.555+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:25:47.558+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:25:47.557+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:25:47.565+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:25:47.563+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:25:47.567+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:25:47.599+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.051 seconds
[2024-03-23T17:26:17.853+0000] {processor.py:161} INFO - Started process (PID=4480) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:26:17.856+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:26:17.861+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:26:17.860+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:26:17.870+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:26:17.868+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:26:17.871+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:26:17.912+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T17:26:48.127+0000] {processor.py:161} INFO - Started process (PID=4496) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:26:48.130+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:26:48.138+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:26:48.136+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:26:48.152+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:26:48.148+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:26:48.156+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:26:48.183+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T17:27:18.387+0000] {processor.py:161} INFO - Started process (PID=4512) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:27:18.388+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:27:18.392+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:27:18.391+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:27:18.400+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:27:18.397+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:27:18.402+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:27:18.451+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T17:27:48.658+0000] {processor.py:161} INFO - Started process (PID=4528) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:27:48.659+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:27:48.662+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:27:48.661+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:27:48.670+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:27:48.667+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:27:48.671+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:27:48.697+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.045 seconds
[2024-03-23T17:28:18.910+0000] {processor.py:161} INFO - Started process (PID=4544) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:28:18.911+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:28:18.914+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:28:18.914+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:28:18.922+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:28:18.920+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:28:18.924+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:28:18.959+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.054 seconds
[2024-03-23T17:28:49.151+0000] {processor.py:161} INFO - Started process (PID=4560) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:28:49.153+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:28:49.157+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:28:49.157+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:28:49.166+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:28:49.164+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:28:49.167+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:28:49.200+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.053 seconds
[2024-03-23T17:29:19.376+0000] {processor.py:161} INFO - Started process (PID=4576) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:29:19.378+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:29:19.384+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:29:19.382+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:29:19.393+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:29:19.390+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:29:19.395+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:29:19.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T17:29:49.636+0000] {processor.py:161} INFO - Started process (PID=4592) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:29:49.639+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:29:49.646+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:29:49.645+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:29:49.657+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:29:49.653+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:29:49.659+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:29:49.698+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.072 seconds
[2024-03-23T17:30:19.910+0000] {processor.py:161} INFO - Started process (PID=4608) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:30:19.912+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:30:19.918+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:30:19.917+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:30:19.928+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:30:19.926+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:30:19.932+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:30:19.976+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.071 seconds
[2024-03-23T17:30:50.188+0000] {processor.py:161} INFO - Started process (PID=4624) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:30:50.190+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:30:50.197+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:30:50.196+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:30:50.208+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:30:50.205+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:30:50.210+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:30:50.241+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T17:31:20.486+0000] {processor.py:161} INFO - Started process (PID=4640) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:31:20.489+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:31:20.495+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:31:20.493+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:31:20.509+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:31:20.506+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:31:20.511+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:31:20.537+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T17:31:50.747+0000] {processor.py:161} INFO - Started process (PID=4656) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:31:50.750+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:31:50.757+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:31:50.755+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:31:50.766+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:31:50.764+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:31:50.767+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:31:50.806+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T17:32:20.998+0000] {processor.py:161} INFO - Started process (PID=4672) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:32:21.000+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:32:21.005+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:32:21.004+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:32:21.013+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:32:21.011+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:32:21.014+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:32:21.042+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.047 seconds
[2024-03-23T17:32:51.258+0000] {processor.py:161} INFO - Started process (PID=4688) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:32:51.260+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:32:51.267+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:32:51.266+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:32:51.276+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:32:51.273+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:32:51.277+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:32:51.308+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.057 seconds
[2024-03-23T17:33:21.519+0000] {processor.py:161} INFO - Started process (PID=4704) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:33:21.522+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:33:21.527+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:33:21.526+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:33:21.539+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:33:21.535+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:33:21.541+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:33:21.582+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.071 seconds
[2024-03-23T17:33:51.769+0000] {processor.py:161} INFO - Started process (PID=4720) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:33:51.772+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:33:51.779+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:33:51.777+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:33:51.791+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:33:51.787+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:33:51.794+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:33:51.843+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.078 seconds
[2024-03-23T17:34:22.055+0000] {processor.py:161} INFO - Started process (PID=4736) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:34:22.057+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:34:22.063+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:34:22.062+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:34:22.075+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:34:22.072+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:34:22.077+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:34:22.106+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T17:34:52.312+0000] {processor.py:161} INFO - Started process (PID=4752) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:34:52.315+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:34:52.319+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:34:52.318+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:34:52.327+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:34:52.325+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:34:52.328+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:34:52.358+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.053 seconds
[2024-03-23T17:35:22.575+0000] {processor.py:161} INFO - Started process (PID=4768) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:35:22.576+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:35:22.581+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:35:22.580+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:35:22.588+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:35:22.586+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:35:22.589+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:35:22.619+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.048 seconds
[2024-03-23T17:35:52.837+0000] {processor.py:161} INFO - Started process (PID=4784) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:35:52.839+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:35:52.845+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:35:52.844+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:35:52.856+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:35:52.853+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:35:52.858+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:35:52.891+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T17:36:23.079+0000] {processor.py:161} INFO - Started process (PID=4800) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:36:23.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:36:23.085+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:36:23.084+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:36:23.098+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:36:23.095+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:36:23.100+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:36:23.142+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.069 seconds
[2024-03-23T17:36:53.345+0000] {processor.py:161} INFO - Started process (PID=4816) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:36:53.347+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:36:53.349+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:36:53.349+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:36:53.355+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:36:53.353+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:36:53.357+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:36:53.388+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.052 seconds
[2024-03-23T17:37:23.547+0000] {processor.py:161} INFO - Started process (PID=4832) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:37:23.550+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:37:23.558+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:37:23.556+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:37:23.567+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:37:23.563+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:37:23.569+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:37:23.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.053 seconds
[2024-03-23T17:37:53.804+0000] {processor.py:161} INFO - Started process (PID=4848) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:37:53.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:37:53.811+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:37:53.810+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:37:53.822+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:37:53.818+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:37:53.823+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:37:53.855+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T17:38:24.045+0000] {processor.py:161} INFO - Started process (PID=4864) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:38:24.046+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:38:24.050+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:38:24.049+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:38:24.059+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:38:24.057+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:38:24.061+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:38:24.086+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.045 seconds
[2024-03-23T17:38:54.260+0000] {processor.py:161} INFO - Started process (PID=4880) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:38:54.263+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:38:54.270+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:38:54.268+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:38:54.282+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:38:54.280+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:38:54.285+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:38:54.309+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.056 seconds
[2024-03-23T17:39:24.488+0000] {processor.py:161} INFO - Started process (PID=4896) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:39:24.489+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:39:24.494+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:39:24.492+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:39:24.505+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:39:24.502+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:39:24.507+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:39:24.554+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T17:39:54.757+0000] {processor.py:161} INFO - Started process (PID=4912) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:39:54.759+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:39:54.763+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:39:54.762+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:39:54.771+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:39:54.768+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:39:54.772+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:39:54.812+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T17:40:24.996+0000] {processor.py:161} INFO - Started process (PID=4928) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:40:24.997+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:40:25.004+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:40:25.003+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:40:25.017+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:40:25.014+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:40:25.018+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:40:25.067+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.076 seconds
[2024-03-23T17:40:55.300+0000] {processor.py:161} INFO - Started process (PID=4944) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:40:55.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:40:55.306+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:40:55.305+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:40:55.315+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:40:55.312+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:40:55.317+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:40:55.346+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.049 seconds
[2024-03-23T17:41:25.515+0000] {processor.py:161} INFO - Started process (PID=4960) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:41:25.518+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:41:25.523+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:41:25.522+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:41:25.531+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:41:25.528+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:41:25.532+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:41:25.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.049 seconds
[2024-03-23T17:41:55.753+0000] {processor.py:161} INFO - Started process (PID=4976) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:41:55.755+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:41:55.760+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:41:55.759+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:41:55.774+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:41:55.772+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:41:55.775+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:41:55.809+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T17:42:25.957+0000] {processor.py:161} INFO - Started process (PID=4992) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:42:25.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:42:25.962+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:42:25.962+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:42:25.973+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:42:25.971+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:42:25.975+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:42:26.014+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T17:42:56.215+0000] {processor.py:161} INFO - Started process (PID=5008) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:42:56.217+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:42:56.224+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:42:56.223+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:42:56.235+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:42:56.233+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:42:56.238+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:42:56.278+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T17:43:26.492+0000] {processor.py:161} INFO - Started process (PID=5024) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:43:26.494+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:43:26.501+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:43:26.500+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:43:26.509+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:43:26.507+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:43:26.511+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:43:26.546+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T17:43:56.763+0000] {processor.py:161} INFO - Started process (PID=5040) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:43:56.765+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:43:56.771+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:43:56.770+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:43:56.783+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:43:56.779+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:43:56.786+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:43:56.821+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T17:44:27.020+0000] {processor.py:161} INFO - Started process (PID=5056) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:44:27.022+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:44:27.030+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:44:27.029+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:44:27.039+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:44:27.037+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:44:27.040+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:44:27.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T17:44:57.292+0000] {processor.py:161} INFO - Started process (PID=5072) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:44:57.293+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:44:57.298+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:44:57.297+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:44:57.311+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:44:57.308+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:44:57.314+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:44:57.355+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T17:45:27.557+0000] {processor.py:161} INFO - Started process (PID=5088) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:45:27.559+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:45:27.564+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:45:27.563+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:45:27.573+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:45:27.570+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:45:27.574+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:45:27.618+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T17:45:57.857+0000] {processor.py:161} INFO - Started process (PID=5104) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:45:57.859+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:45:57.863+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:45:57.862+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:45:57.873+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:45:57.870+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:45:57.874+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:45:57.902+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.051 seconds
[2024-03-23T17:46:28.076+0000] {processor.py:161} INFO - Started process (PID=5120) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:46:28.078+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:46:28.085+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:46:28.083+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:46:28.096+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:46:28.094+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:46:28.097+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:46:28.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.065 seconds
[2024-03-23T17:46:58.336+0000] {processor.py:161} INFO - Started process (PID=5136) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:46:58.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:46:58.341+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:46:58.341+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:46:58.351+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:46:58.348+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:46:58.353+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:46:58.382+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.049 seconds
[2024-03-23T17:47:28.562+0000] {processor.py:161} INFO - Started process (PID=5152) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:47:28.564+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:47:28.571+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:47:28.569+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:47:28.586+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:47:28.581+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:47:28.589+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:47:28.624+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.069 seconds
[2024-03-23T17:47:58.829+0000] {processor.py:161} INFO - Started process (PID=5168) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:47:58.831+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:47:58.834+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:47:58.833+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:47:58.844+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:47:58.841+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:47:58.846+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:47:58.890+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T17:48:29.097+0000] {processor.py:161} INFO - Started process (PID=5184) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:48:29.099+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:48:29.104+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:48:29.103+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:48:29.113+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:48:29.111+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:48:29.115+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:48:29.157+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T17:48:59.366+0000] {processor.py:161} INFO - Started process (PID=5200) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:48:59.369+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:48:59.372+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:48:59.371+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:48:59.384+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:48:59.380+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:48:59.386+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:48:59.416+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T17:49:29.626+0000] {processor.py:161} INFO - Started process (PID=5216) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:49:29.629+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:49:29.637+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:49:29.636+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:49:29.649+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:49:29.646+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:49:29.651+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:49:29.678+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.058 seconds
[2024-03-23T17:49:59.851+0000] {processor.py:161} INFO - Started process (PID=5232) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:49:59.853+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:49:59.861+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:49:59.860+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:49:59.872+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:49:59.869+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:49:59.874+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:49:59.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T17:50:30.056+0000] {processor.py:161} INFO - Started process (PID=5248) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:50:30.058+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:50:30.063+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:50:30.062+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:50:30.072+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:50:30.069+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:50:30.074+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:50:30.110+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T17:51:00.328+0000] {processor.py:161} INFO - Started process (PID=5264) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:51:00.330+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:51:00.336+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:51:00.335+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:51:00.348+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:51:00.345+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:51:00.351+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:51:00.387+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T17:51:30.590+0000] {processor.py:161} INFO - Started process (PID=5281) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:51:30.593+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:51:30.599+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:51:30.598+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:51:30.609+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:51:30.606+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:51:30.610+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:51:30.637+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.056 seconds
[2024-03-23T17:52:00.855+0000] {processor.py:161} INFO - Started process (PID=5297) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:52:00.857+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:52:00.863+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:52:00.862+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:52:00.876+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:52:00.872+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:52:00.878+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:52:00.926+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.078 seconds
[2024-03-23T17:52:31.101+0000] {processor.py:161} INFO - Started process (PID=5312) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:52:31.103+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:52:31.107+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:52:31.106+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:52:31.117+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:52:31.115+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:52:31.119+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:52:31.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T17:53:01.366+0000] {processor.py:161} INFO - Started process (PID=5328) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:53:01.369+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:53:01.375+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:53:01.373+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:53:01.383+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:53:01.381+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:53:01.384+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:53:01.417+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T17:53:31.619+0000] {processor.py:161} INFO - Started process (PID=5344) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:53:31.622+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:53:31.627+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:53:31.626+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:53:31.639+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:53:31.636+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:53:31.641+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:53:31.692+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.083 seconds
[2024-03-23T17:54:01.876+0000] {processor.py:161} INFO - Started process (PID=5360) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:54:01.878+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:54:01.883+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:54:01.883+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:54:01.892+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:54:01.890+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:54:01.895+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:54:01.931+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T17:54:32.160+0000] {processor.py:161} INFO - Started process (PID=5376) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:54:32.163+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:54:32.167+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:54:32.166+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:54:32.177+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:54:32.174+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:54:32.179+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:54:32.221+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T17:55:02.431+0000] {processor.py:161} INFO - Started process (PID=5392) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:55:02.433+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:55:02.440+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:55:02.438+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:55:02.449+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:55:02.447+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:55:02.450+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:55:02.487+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T17:55:32.687+0000] {processor.py:161} INFO - Started process (PID=5408) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:55:32.689+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:55:32.695+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:55:32.694+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:55:32.705+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:55:32.702+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:55:32.706+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:55:32.742+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T17:56:02.989+0000] {processor.py:161} INFO - Started process (PID=5424) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:56:02.992+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:56:02.998+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:56:02.997+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:56:03.009+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:56:03.007+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:56:03.011+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:56:03.045+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T17:56:33.235+0000] {processor.py:161} INFO - Started process (PID=5440) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:56:33.236+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:56:33.241+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:56:33.240+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:56:33.250+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:56:33.248+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:56:33.251+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:56:33.289+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T17:57:03.499+0000] {processor.py:161} INFO - Started process (PID=5456) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:57:03.501+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:57:03.504+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:57:03.504+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:57:03.515+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:57:03.512+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:57:03.517+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:57:03.555+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T17:57:33.747+0000] {processor.py:161} INFO - Started process (PID=5472) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:57:33.750+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:57:33.757+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:57:33.756+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:57:33.769+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:57:33.765+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:57:33.771+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:57:33.800+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T17:58:03.971+0000] {processor.py:161} INFO - Started process (PID=5488) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:58:03.974+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:58:03.982+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:58:03.980+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:58:03.994+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:58:03.991+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:58:03.997+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:58:04.035+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.073 seconds
[2024-03-23T17:58:34.248+0000] {processor.py:161} INFO - Started process (PID=5504) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:58:34.250+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:58:34.256+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:58:34.255+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:58:34.270+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:58:34.266+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:58:34.272+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:58:34.305+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.064 seconds
[2024-03-23T17:59:04.543+0000] {processor.py:161} INFO - Started process (PID=5520) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:59:04.545+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:59:04.552+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:59:04.550+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:59:04.567+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:59:04.562+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:59:04.570+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:59:04.616+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.081 seconds
[2024-03-23T17:59:34.829+0000] {processor.py:161} INFO - Started process (PID=5536) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T17:59:34.832+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T17:59:34.839+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:59:34.838+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T17:59:34.846+0000] {logging_mixin.py:188} INFO - [2024-03-23T17:59:34.845+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T17:59:34.847+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T17:59:34.881+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T18:00:05.110+0000] {processor.py:161} INFO - Started process (PID=5552) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:00:05.111+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:00:05.115+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:00:05.115+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:00:05.122+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:00:05.120+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:00:05.123+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:00:05.155+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.049 seconds
[2024-03-23T18:00:35.383+0000] {processor.py:161} INFO - Started process (PID=5568) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:00:35.385+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:00:35.392+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:00:35.391+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:00:35.403+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:00:35.400+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:00:35.405+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:00:35.437+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T18:01:05.644+0000] {processor.py:161} INFO - Started process (PID=5584) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:01:05.647+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:01:05.653+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:01:05.652+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:01:05.667+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:01:05.662+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:01:05.670+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:01:05.709+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.072 seconds
[2024-03-23T18:01:35.909+0000] {processor.py:161} INFO - Started process (PID=5600) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:01:35.911+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:01:35.915+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:01:35.914+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:01:35.924+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:01:35.922+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:01:35.926+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:01:35.973+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.068 seconds
[2024-03-23T18:02:06.168+0000] {processor.py:161} INFO - Started process (PID=5616) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:02:06.170+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:02:06.173+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:02:06.172+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:02:06.181+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:02:06.179+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:02:06.183+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:02:06.211+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.046 seconds
[2024-03-23T18:02:36.396+0000] {processor.py:161} INFO - Started process (PID=5632) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:02:36.399+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:02:36.406+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:02:36.406+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:02:36.415+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:02:36.413+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:02:36.417+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:02:36.454+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.066 seconds
[2024-03-23T18:03:06.679+0000] {processor.py:161} INFO - Started process (PID=5648) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:03:06.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:03:06.689+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:03:06.687+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:03:06.697+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:03:06.696+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:03:06.698+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:03:06.733+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.063 seconds
[2024-03-23T18:03:36.953+0000] {processor.py:161} INFO - Started process (PID=5664) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:03:36.954+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:03:36.959+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:03:36.958+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:03:36.972+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:03:36.968+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:03:36.974+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:03:37.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T18:04:07.187+0000] {processor.py:161} INFO - Started process (PID=5680) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:04:07.190+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:04:07.196+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:04:07.195+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:04:07.206+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:04:07.203+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:04:07.207+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:04:07.240+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T18:04:37.466+0000] {processor.py:161} INFO - Started process (PID=5696) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:04:37.468+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:04:37.477+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:04:37.475+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:04:37.489+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:04:37.485+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:04:37.491+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:04:37.529+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T18:05:07.754+0000] {processor.py:161} INFO - Started process (PID=5712) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:05:07.756+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:05:07.763+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:05:07.762+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:05:07.770+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:05:07.769+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:05:07.771+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:05:07.803+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.057 seconds
[2024-03-23T18:05:38.023+0000] {processor.py:161} INFO - Started process (PID=5728) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:05:38.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:05:38.031+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:05:38.030+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:05:38.043+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:05:38.038+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:05:38.045+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:05:38.077+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T18:06:08.328+0000] {processor.py:161} INFO - Started process (PID=5744) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:06:08.331+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:06:08.337+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:06:08.336+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:06:08.353+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:06:08.349+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:06:08.355+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:06:08.390+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.069 seconds
[2024-03-23T18:06:38.561+0000] {processor.py:161} INFO - Started process (PID=5760) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:06:38.564+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:06:38.569+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:06:38.569+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:06:38.580+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:06:38.577+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:06:38.582+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:06:38.624+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.071 seconds
[2024-03-23T18:07:08.791+0000] {processor.py:161} INFO - Started process (PID=5776) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:07:08.793+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:07:08.801+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:07:08.799+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:07:08.816+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:07:08.811+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:07:08.819+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:07:08.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.082 seconds
[2024-03-23T18:07:39.066+0000] {processor.py:161} INFO - Started process (PID=5792) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:07:39.068+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:07:39.073+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:07:39.072+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:07:39.083+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:07:39.081+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:07:39.085+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:07:39.128+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T18:08:09.346+0000] {processor.py:161} INFO - Started process (PID=5808) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:08:09.348+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:08:09.355+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:08:09.354+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:08:09.364+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:08:09.362+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:08:09.365+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:08:09.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T18:08:39.599+0000] {processor.py:161} INFO - Started process (PID=5824) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:08:39.600+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:08:39.603+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:08:39.603+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:08:39.611+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:08:39.608+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:08:39.613+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:08:39.652+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.059 seconds
[2024-03-23T18:09:09.836+0000] {processor.py:161} INFO - Started process (PID=5840) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:09:09.839+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:09:09.842+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:09:09.842+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:09:09.849+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:09:09.848+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:09:09.850+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:09:09.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.060 seconds
[2024-03-23T18:09:40.076+0000] {processor.py:161} INFO - Started process (PID=5856) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:09:40.078+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:09:40.081+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:09:40.080+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:09:40.090+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:09:40.088+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:09:40.092+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:09:40.133+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T18:10:10.366+0000] {processor.py:161} INFO - Started process (PID=5872) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:10:10.368+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:10:10.372+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:10:10.371+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:10:10.388+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:10:10.386+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:10:10.389+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:10:10.426+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.070 seconds
[2024-03-23T18:10:40.634+0000] {processor.py:161} INFO - Started process (PID=5888) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:10:40.635+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:10:40.639+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:10:40.639+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:10:40.653+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:10:40.649+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:10:40.654+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:10:40.691+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.062 seconds
[2024-03-23T18:11:10.946+0000] {processor.py:161} INFO - Started process (PID=5904) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:11:10.948+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:11:10.952+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:11:10.951+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:11:10.960+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:11:10.957+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:11:10.964+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:11:10.999+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.057 seconds
[2024-03-23T18:11:41.229+0000] {processor.py:161} INFO - Started process (PID=5920) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:11:41.233+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:11:41.240+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:11:41.239+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:11:41.266+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:11:41.257+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:11:41.271+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:11:41.326+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.105 seconds
[2024-03-23T18:12:11.571+0000] {processor.py:161} INFO - Started process (PID=5936) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:12:11.573+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:12:11.580+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:12:11.579+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:12:11.590+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:12:11.588+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:12:11.591+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:12:11.624+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.061 seconds
[2024-03-23T18:12:41.776+0000] {processor.py:161} INFO - Started process (PID=5952) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:12:41.779+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:12:41.783+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:12:41.782+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:12:41.795+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:12:41.793+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:12:41.797+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:12:41.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.076 seconds
[2024-03-23T18:13:11.989+0000] {processor.py:161} INFO - Started process (PID=5968) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:13:11.991+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:13:11.997+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:13:11.996+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:13:12.013+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:13:12.008+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:13:12.017+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:13:12.057+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.075 seconds
[2024-03-23T18:13:42.208+0000] {processor.py:161} INFO - Started process (PID=5989) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:13:42.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:13:42.212+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:13:42.211+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:13:42.219+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:13:42.217+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 4, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator,BigQueryCreateEmptyTableOperator,BigQueryIn
ImportError: cannot import name 'BigQueryIn' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-03-23T18:13:42.220+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:13:42.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.050 seconds
[2024-03-23T18:13:56.369+0000] {processor.py:161} INFO - Started process (PID=6005) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:13:56.370+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:13:56.373+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:13:56.373+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:13:56.399+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:13:56.541+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:13:56.540+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:13:56.565+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:13:56.564+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T18:13:56.590+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.225 seconds
[2024-03-23T18:14:27.564+0000] {processor.py:161} INFO - Started process (PID=6031) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:14:27.565+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:14:27.568+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:14:27.567+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:14:27.588+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:14:27.621+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:14:27.620+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:14:27.651+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:14:27.651+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:14:27.683+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.123 seconds
[2024-03-23T18:14:57.820+0000] {processor.py:161} INFO - Started process (PID=6047) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:14:57.821+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:14:57.825+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:14:57.824+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:14:57.843+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:14:57.875+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:14:57.874+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:14:57.909+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:14:57.909+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:14:57.950+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.135 seconds
[2024-03-23T18:15:28.061+0000] {processor.py:161} INFO - Started process (PID=6063) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:15:28.063+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:15:28.066+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:15:28.065+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:15:28.093+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:15:28.130+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:15:28.129+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:15:28.172+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:15:28.172+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:15:28.199+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.143 seconds
[2024-03-23T18:15:58.326+0000] {processor.py:161} INFO - Started process (PID=6080) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:15:58.328+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:15:58.332+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:15:58.331+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:15:58.356+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:15:58.391+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:15:58.390+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:15:58.418+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:15:58.418+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:15:58.451+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.132 seconds
[2024-03-23T18:16:28.546+0000] {processor.py:161} INFO - Started process (PID=6096) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:16:28.547+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:16:28.550+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:16:28.549+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:16:28.565+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:16:28.592+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:16:28.591+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:16:28.638+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:16:28.637+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:16:28.679+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.137 seconds
[2024-03-23T18:16:58.810+0000] {processor.py:161} INFO - Started process (PID=6112) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:16:58.811+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:16:58.816+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:16:58.814+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:16:58.857+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:16:58.893+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:16:58.892+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:16:58.920+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:16:58.920+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:16:58.941+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.135 seconds
[2024-03-23T18:17:29.041+0000] {processor.py:161} INFO - Started process (PID=6128) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:17:29.042+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:17:29.045+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:17:29.044+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:17:29.061+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:17:29.105+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:17:29.104+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:17:29.134+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:17:29.134+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:17:29.170+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.135 seconds
[2024-03-23T18:17:59.365+0000] {processor.py:161} INFO - Started process (PID=6144) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:17:59.366+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:17:59.369+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:17:59.369+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:17:59.389+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:17:59.421+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:17:59.420+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:17:59.452+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:17:59.451+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:17:59.485+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.123 seconds
[2024-03-23T18:18:29.576+0000] {processor.py:161} INFO - Started process (PID=6160) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:18:29.578+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:18:29.582+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:18:29.581+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:18:29.608+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:18:29.651+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:18:29.650+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:18:29.693+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:18:29.693+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:18:29.726+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.155 seconds
[2024-03-23T18:18:59.834+0000] {processor.py:161} INFO - Started process (PID=6176) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:18:59.836+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:18:59.843+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:18:59.842+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:18:59.880+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:18:59.921+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:18:59.919+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:18:59.952+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:18:59.951+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:18:59.978+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.152 seconds
[2024-03-23T18:19:30.085+0000] {processor.py:161} INFO - Started process (PID=6195) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:19:30.087+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:19:30.094+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:19:30.093+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:19:30.134+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:19:30.159+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:19:30.158+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:19:30.184+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:19:30.184+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:19:30.215+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.135 seconds
[2024-03-23T18:20:00.321+0000] {processor.py:161} INFO - Started process (PID=6211) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:20:00.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:20:00.325+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:20:00.325+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:20:00.341+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:20:00.367+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:20:00.366+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:20:00.393+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:20:00.393+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:20:00.424+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.107 seconds
[2024-03-23T18:20:30.611+0000] {processor.py:161} INFO - Started process (PID=6227) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:20:30.612+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:20:30.616+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:20:30.615+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:20:30.636+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:20:30.667+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:20:30.666+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:20:30.703+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:20:30.702+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:20:30.739+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.133 seconds
[2024-03-23T18:21:00.835+0000] {processor.py:161} INFO - Started process (PID=6244) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:21:00.836+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:21:00.840+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:21:00.839+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:21:00.873+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:21:00.915+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:21:00.915+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:21:00.967+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:21:00.966+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:21:01.003+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.172 seconds
[2024-03-23T18:21:31.072+0000] {processor.py:161} INFO - Started process (PID=6260) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:21:31.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:21:31.076+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:21:31.076+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:21:31.093+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:21:31.124+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:21:31.123+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:21:31.159+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:21:31.158+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:21:31.195+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.128 seconds
[2024-03-23T18:22:01.232+0000] {processor.py:161} INFO - Started process (PID=6276) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:22:01.233+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:22:01.236+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:22:01.235+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:22:01.253+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:22:01.291+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:22:01.290+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:22:01.321+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:22:01.320+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:22:01.354+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.126 seconds
[2024-03-23T18:22:31.436+0000] {processor.py:161} INFO - Started process (PID=6292) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:22:31.437+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:22:31.440+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:22:31.440+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:22:31.458+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:22:31.488+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:22:31.487+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:22:31.519+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:22:31.519+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:22:31.544+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.111 seconds
[2024-03-23T18:23:01.634+0000] {processor.py:161} INFO - Started process (PID=6308) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:23:01.636+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:23:01.639+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:23:01.638+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:23:01.657+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:23:01.688+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:23:01.688+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:23:01.720+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:23:01.719+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:23:01.754+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.123 seconds
[2024-03-23T18:23:31.920+0000] {processor.py:161} INFO - Started process (PID=6324) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:23:31.921+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:23:31.925+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:23:31.924+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:23:31.947+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:23:31.981+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:23:31.980+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:23:32.016+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:23:32.015+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:23:32.066+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.151 seconds
[2024-03-23T18:24:02.190+0000] {processor.py:161} INFO - Started process (PID=6340) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:24:02.192+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:24:02.197+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:24:02.196+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:24:02.232+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:24:02.284+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:24:02.284+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:24:02.313+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:24:02.312+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:24:02.346+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.163 seconds
[2024-03-23T18:24:32.446+0000] {processor.py:161} INFO - Started process (PID=6356) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:24:32.447+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:24:32.451+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:24:32.450+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:24:32.470+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:24:32.503+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:24:32.502+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:24:32.535+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:24:32.535+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:24:32.576+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.134 seconds
[2024-03-23T18:25:02.642+0000] {processor.py:161} INFO - Started process (PID=6372) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:25:02.645+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:25:02.653+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:25:02.651+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:25:02.678+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:25:02.724+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:25:02.723+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:25:02.766+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:25:02.765+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:25:02.799+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.167 seconds
[2024-03-23T18:25:32.870+0000] {processor.py:161} INFO - Started process (PID=6388) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:25:32.871+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:25:32.875+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:25:32.874+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:25:32.918+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:25:32.962+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:25:32.961+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:25:32.992+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:25:32.991+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:25:33.030+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.166 seconds
[2024-03-23T18:26:03.138+0000] {processor.py:161} INFO - Started process (PID=6404) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:26:03.140+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:26:03.145+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:26:03.144+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:26:03.178+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:26:03.220+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:26:03.220+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:26:03.248+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:26:03.248+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:26:03.279+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.147 seconds
[2024-03-23T18:26:33.358+0000] {processor.py:161} INFO - Started process (PID=6420) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:26:33.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:26:33.364+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:26:33.364+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:26:33.381+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:26:33.411+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:26:33.411+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:26:33.443+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:26:33.442+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:26:33.473+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.123 seconds
[2024-03-23T18:27:03.679+0000] {processor.py:161} INFO - Started process (PID=6436) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:27:03.681+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:27:03.684+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:27:03.683+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:27:03.707+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:27:03.740+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:27:03.740+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:27:03.770+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:27:03.769+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:27:03.802+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.130 seconds
[2024-03-23T18:27:33.906+0000] {processor.py:161} INFO - Started process (PID=6452) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:27:33.908+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:27:33.911+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:27:33.910+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:27:33.928+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:27:33.955+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:27:33.954+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:27:33.980+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:27:33.979+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:27:34.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.099 seconds
[2024-03-23T18:28:04.189+0000] {processor.py:161} INFO - Started process (PID=6468) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:28:04.191+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:28:04.197+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:28:04.195+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:28:04.230+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:28:04.258+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:28:04.258+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:28:04.292+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:28:04.292+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:28:04.318+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.136 seconds
[2024-03-23T18:28:34.416+0000] {processor.py:161} INFO - Started process (PID=6485) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:28:34.418+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:28:34.428+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:28:34.427+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:28:34.456+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:28:34.498+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:28:34.497+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:28:34.529+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:28:34.529+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:28:34.562+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.154 seconds
[2024-03-23T18:29:04.636+0000] {processor.py:161} INFO - Started process (PID=6501) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:29:04.637+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:29:04.641+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:29:04.640+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:29:04.658+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:29:04.686+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:29:04.685+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:29:04.716+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:29:04.716+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:29:04.751+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.119 seconds
[2024-03-23T18:29:34.950+0000] {processor.py:161} INFO - Started process (PID=6516) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:29:34.953+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:29:34.959+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:29:34.958+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:29:34.990+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:29:35.048+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:29:35.046+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:29:35.082+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:29:35.082+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:29:35.112+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.168 seconds
[2024-03-23T18:30:05.184+0000] {processor.py:161} INFO - Started process (PID=6532) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:30:05.185+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:30:05.190+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:30:05.189+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:30:05.211+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:30:05.242+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:30:05.242+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:30:05.275+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:30:05.275+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:30:05.311+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.130 seconds
[2024-03-23T18:30:35.357+0000] {processor.py:161} INFO - Started process (PID=6548) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:30:35.358+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:30:35.362+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:30:35.361+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:30:35.381+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:30:35.411+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:30:35.410+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:30:35.439+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:30:35.439+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:30:35.471+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.118 seconds
[2024-03-23T18:31:05.594+0000] {processor.py:161} INFO - Started process (PID=6564) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:31:05.596+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:31:05.599+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:31:05.598+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:31:05.616+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:31:05.645+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:31:05.644+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:31:05.674+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:31:05.674+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:31:05.709+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.118 seconds
[2024-03-23T18:31:35.807+0000] {processor.py:161} INFO - Started process (PID=6580) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:31:35.809+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:31:35.813+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:31:35.812+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:31:35.832+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:31:35.872+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:31:35.871+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:31:35.901+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:31:35.900+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:31:35.938+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.139 seconds
[2024-03-23T18:32:06.039+0000] {processor.py:161} INFO - Started process (PID=6597) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:32:06.041+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:32:06.046+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:32:06.045+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:32:06.070+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:32:06.100+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:32:06.099+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:32:06.130+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:32:06.129+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:32:06.166+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.132 seconds
[2024-03-23T18:32:36.255+0000] {processor.py:161} INFO - Started process (PID=6613) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:32:36.257+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:32:36.262+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:32:36.261+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:32:36.296+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:32:36.340+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:32:36.340+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:32:36.368+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:32:36.368+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:32:36.402+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.150 seconds
[2024-03-23T18:33:06.465+0000] {processor.py:161} INFO - Started process (PID=6629) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:33:06.467+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:33:06.470+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:33:06.469+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:33:06.487+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:33:06.516+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:33:06.516+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:33:06.547+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:33:06.547+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:33:06.581+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.119 seconds
[2024-03-23T18:33:36.677+0000] {processor.py:161} INFO - Started process (PID=6645) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:33:36.678+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:33:36.682+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:33:36.681+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:33:36.705+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:33:36.740+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:33:36.739+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:33:36.772+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:33:36.772+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:33:36.809+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.136 seconds
[2024-03-23T18:34:06.900+0000] {processor.py:161} INFO - Started process (PID=6661) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:34:06.902+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:34:06.905+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:34:06.905+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:34:06.928+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:34:06.965+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:34:06.964+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:34:06.998+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:34:06.998+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:34:07.034+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.138 seconds
[2024-03-23T18:34:37.137+0000] {processor.py:161} INFO - Started process (PID=6677) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:34:37.138+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:34:37.142+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:34:37.141+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:34:37.162+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:34:37.196+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:34:37.195+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:34:37.238+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:34:37.238+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:34:37.278+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.145 seconds
[2024-03-23T18:35:07.331+0000] {processor.py:161} INFO - Started process (PID=6693) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:35:07.333+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:35:07.337+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:35:07.336+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:35:07.357+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:35:07.391+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:35:07.390+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:35:07.428+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:35:07.428+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:35:07.467+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.140 seconds
[2024-03-23T18:35:37.556+0000] {processor.py:161} INFO - Started process (PID=6709) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:35:37.557+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:35:37.560+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:35:37.559+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:35:37.578+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:35:37.605+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:35:37.605+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:35:37.635+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:35:37.634+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:35:37.669+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.117 seconds
[2024-03-23T18:36:07.773+0000] {processor.py:161} INFO - Started process (PID=6725) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:36:07.774+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:36:07.780+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:36:07.778+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:36:07.813+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:36:07.847+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:36:07.846+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:36:07.881+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:36:07.880+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:36:07.915+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.146 seconds
[2024-03-23T18:36:37.973+0000] {processor.py:161} INFO - Started process (PID=6741) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:36:37.975+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:36:37.978+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:36:37.977+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:36:38.011+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:36:38.065+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:36:38.064+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:36:38.096+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:36:38.096+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:36:38.130+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.160 seconds
[2024-03-23T18:37:08.186+0000] {processor.py:161} INFO - Started process (PID=6757) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:37:08.187+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:37:08.191+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:37:08.190+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:37:08.208+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:37:08.238+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:37:08.238+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:37:08.273+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:37:08.273+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:37:08.311+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.129 seconds
[2024-03-23T18:37:38.418+0000] {processor.py:161} INFO - Started process (PID=6773) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:37:38.419+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:37:38.423+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:37:38.422+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:37:38.442+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:37:38.473+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:37:38.472+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:37:38.506+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:37:38.506+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:37:38.535+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.120 seconds
[2024-03-23T18:38:08.623+0000] {processor.py:161} INFO - Started process (PID=6789) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:38:08.624+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:38:08.627+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:38:08.626+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:38:08.647+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:38:08.682+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:38:08.682+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:38:08.712+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:38:08.712+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:38:08.737+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.118 seconds
[2024-03-23T18:38:38.850+0000] {processor.py:161} INFO - Started process (PID=6805) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:38:38.851+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:38:38.855+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:38:38.854+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:38:38.881+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:38:38.922+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:38:38.921+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:38:38.958+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:38:38.958+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:38:38.992+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.147 seconds
[2024-03-23T18:39:09.058+0000] {processor.py:161} INFO - Started process (PID=6821) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:39:09.062+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:39:09.067+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:39:09.066+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:39:09.098+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:39:09.130+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:39:09.129+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:39:09.179+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:39:09.179+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:39:09.235+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.184 seconds
[2024-03-23T18:39:39.301+0000] {processor.py:161} INFO - Started process (PID=6837) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:39:39.303+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:39:39.307+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:39:39.306+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:39:39.333+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:39:39.375+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:39:39.374+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:39:39.410+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:39:39.410+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:39:39.447+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.150 seconds
[2024-03-23T18:40:09.507+0000] {processor.py:161} INFO - Started process (PID=6853) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:40:09.509+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:40:09.513+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:40:09.512+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:40:09.534+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:40:09.571+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:40:09.570+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:40:09.605+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:40:09.605+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:40:09.644+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.142 seconds
[2024-03-23T18:40:39.713+0000] {processor.py:161} INFO - Started process (PID=6869) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:40:39.714+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:40:39.717+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:40:39.716+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:40:39.735+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:40:39.772+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:40:39.771+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:40:39.810+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:40:39.810+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:40:39.838+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.129 seconds
[2024-03-23T18:41:09.892+0000] {processor.py:161} INFO - Started process (PID=6885) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:41:09.893+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:41:09.897+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:41:09.896+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:41:09.918+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:41:09.952+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:41:09.951+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:41:09.987+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:41:09.987+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:41:10.019+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.131 seconds
[2024-03-23T18:41:40.113+0000] {processor.py:161} INFO - Started process (PID=6901) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:41:40.115+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:41:40.120+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:41:40.119+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:41:40.141+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:41:40.177+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:41:40.176+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:41:40.210+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:41:40.210+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:41:40.249+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.140 seconds
[2024-03-23T18:42:10.324+0000] {processor.py:161} INFO - Started process (PID=6917) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:42:10.326+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:42:10.331+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:42:10.330+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:42:10.354+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:42:10.390+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:42:10.389+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:42:10.425+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:42:10.425+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:42:10.463+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.142 seconds
[2024-03-23T18:42:40.528+0000] {processor.py:161} INFO - Started process (PID=6933) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:42:40.529+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:42:40.533+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:42:40.533+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:42:40.552+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:42:40.584+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:42:40.583+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:42:40.618+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:42:40.618+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:42:40.648+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.125 seconds
[2024-03-23T18:43:10.725+0000] {processor.py:161} INFO - Started process (PID=6949) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:43:10.726+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:43:10.730+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:43:10.729+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:43:10.750+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:43:10.784+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:43:10.783+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:43:10.819+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:43:10.819+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:43:10.856+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.135 seconds
[2024-03-23T18:43:40.945+0000] {processor.py:161} INFO - Started process (PID=6965) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:43:40.946+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:43:40.949+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:43:40.949+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:43:40.970+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:43:41.003+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:43:41.002+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:43:41.036+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:43:41.035+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:43:41.073+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.133 seconds
[2024-03-23T18:44:11.153+0000] {processor.py:161} INFO - Started process (PID=6981) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:44:11.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:44:11.157+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:44:11.157+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:44:11.177+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:44:11.211+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:44:11.210+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:44:11.244+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:44:11.243+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:44:11.280+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.131 seconds
[2024-03-23T18:44:41.381+0000] {processor.py:161} INFO - Started process (PID=6997) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:44:41.383+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:44:41.385+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:44:41.385+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:44:41.406+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:44:41.443+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:44:41.442+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:44:41.480+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:44:41.480+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:44:41.507+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.130 seconds
[2024-03-23T18:45:11.601+0000] {processor.py:161} INFO - Started process (PID=7013) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:45:11.602+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:45:11.606+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:45:11.605+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:45:11.638+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:45:11.681+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:45:11.680+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:45:11.725+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:45:11.724+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:45:11.767+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.171 seconds
[2024-03-23T18:45:41.816+0000] {processor.py:161} INFO - Started process (PID=7029) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:45:41.818+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:45:41.821+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:45:41.820+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:45:41.840+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:45:41.869+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:45:41.869+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:45:41.901+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:45:41.901+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:45:41.942+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.129 seconds
[2024-03-23T18:46:12.045+0000] {processor.py:161} INFO - Started process (PID=7045) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:46:12.046+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:46:12.049+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:46:12.049+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:46:12.070+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:46:12.106+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:46:12.105+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:46:12.140+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:46:12.139+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:46:12.171+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.131 seconds
[2024-03-23T18:46:42.260+0000] {processor.py:161} INFO - Started process (PID=7061) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:46:42.261+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:46:42.265+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:46:42.264+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:46:42.285+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:46:42.316+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:46:42.315+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:46:42.348+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:46:42.347+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:46:42.374+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.119 seconds
[2024-03-23T18:47:12.466+0000] {processor.py:161} INFO - Started process (PID=7077) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:47:12.467+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:47:12.471+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:47:12.470+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:47:12.488+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:47:12.521+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:47:12.520+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:47:12.552+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:47:12.552+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:47:12.587+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.125 seconds
[2024-03-23T18:47:42.686+0000] {processor.py:161} INFO - Started process (PID=7093) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:47:42.687+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:47:42.690+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:47:42.690+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:47:42.710+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:47:42.742+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:47:42.741+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:47:42.773+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:47:42.773+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:47:42.808+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.126 seconds
[2024-03-23T18:48:12.897+0000] {processor.py:161} INFO - Started process (PID=7109) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:48:12.899+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:48:12.903+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:48:12.902+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:48:12.927+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:48:12.958+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:48:12.957+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:48:12.992+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:48:12.992+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:48:13.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.136 seconds
[2024-03-23T18:48:43.086+0000] {processor.py:161} INFO - Started process (PID=7125) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:48:43.087+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:48:43.091+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:48:43.090+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:48:43.112+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:48:43.143+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:48:43.143+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:48:43.174+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:48:43.174+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:48:43.210+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.128 seconds
[2024-03-23T18:49:13.287+0000] {processor.py:161} INFO - Started process (PID=7141) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:49:13.288+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:49:13.292+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:49:13.291+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:49:13.310+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:49:13.341+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:49:13.340+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:49:13.372+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:49:13.372+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:49:13.408+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.125 seconds
[2024-03-23T18:49:43.491+0000] {processor.py:161} INFO - Started process (PID=7157) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:49:43.493+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:49:43.496+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:49:43.495+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:49:43.515+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:49:43.548+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:49:43.547+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:49:43.585+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:49:43.584+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:49:43.620+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.132 seconds
[2024-03-23T18:50:13.690+0000] {processor.py:161} INFO - Started process (PID=7173) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:50:13.692+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:50:13.695+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:50:13.694+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:50:13.714+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:50:13.742+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:50:13.742+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:50:13.772+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:50:13.772+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:50:13.806+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.119 seconds
[2024-03-23T18:50:43.892+0000] {processor.py:161} INFO - Started process (PID=7189) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:50:43.894+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:50:43.897+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:50:43.896+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:50:43.924+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:50:43.958+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:50:43.958+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:50:43.992+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:50:43.991+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:50:44.018+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.130 seconds
[2024-03-23T18:51:14.108+0000] {processor.py:161} INFO - Started process (PID=7205) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:51:14.109+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:51:14.113+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:51:14.113+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:51:14.137+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:51:14.169+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:51:14.168+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:51:14.200+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:51:14.200+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:51:14.239+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.135 seconds
[2024-03-23T18:51:44.308+0000] {processor.py:161} INFO - Started process (PID=7221) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:51:44.309+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:51:44.313+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:51:44.312+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:51:44.332+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:51:44.362+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:51:44.361+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:51:44.392+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:51:44.392+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:51:44.426+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.122 seconds
[2024-03-23T18:52:14.509+0000] {processor.py:161} INFO - Started process (PID=7237) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:52:14.511+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:52:14.515+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:52:14.514+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:52:14.533+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:52:14.563+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:52:14.562+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:52:14.593+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:52:14.592+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:52:14.620+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.114 seconds
[2024-03-23T18:52:44.707+0000] {processor.py:161} INFO - Started process (PID=7253) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:52:44.709+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:52:44.713+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:52:44.713+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:52:44.731+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:52:44.763+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:52:44.763+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:52:44.796+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:52:44.795+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:52:44.820+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.118 seconds
[2024-03-23T18:53:14.913+0000] {processor.py:161} INFO - Started process (PID=7269) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:53:14.914+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:53:14.919+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:53:14.918+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:53:14.947+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:53:14.987+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:53:14.986+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:53:15.020+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:53:15.020+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:53:15.072+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.165 seconds
[2024-03-23T18:53:45.135+0000] {processor.py:161} INFO - Started process (PID=7285) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:53:45.136+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:53:45.141+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:53:45.140+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:53:45.163+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:53:45.199+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:53:45.199+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:53:45.236+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:53:45.235+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:53:45.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.144 seconds
[2024-03-23T18:54:15.354+0000] {processor.py:161} INFO - Started process (PID=7301) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:54:15.355+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:54:15.359+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:54:15.359+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:54:15.378+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:54:15.409+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:54:15.409+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:54:15.443+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:54:15.443+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:54:15.481+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.130 seconds
[2024-03-23T18:54:45.558+0000] {processor.py:161} INFO - Started process (PID=7317) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:54:45.559+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:54:45.563+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:54:45.562+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:54:45.582+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:54:45.613+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:54:45.612+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:54:45.645+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:54:45.644+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:54:45.671+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.116 seconds
[2024-03-23T18:55:15.756+0000] {processor.py:161} INFO - Started process (PID=7333) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:55:15.757+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:55:15.761+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:55:15.760+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:55:15.784+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:55:15.818+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:55:15.817+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:55:15.851+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:55:15.851+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:55:15.878+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.127 seconds
[2024-03-23T18:55:45.960+0000] {processor.py:161} INFO - Started process (PID=7349) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:55:45.963+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:55:45.967+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:55:45.967+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:55:45.986+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:55:46.018+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:55:46.017+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:55:46.048+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:55:46.048+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:55:46.082+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.126 seconds
[2024-03-23T18:56:16.172+0000] {processor.py:161} INFO - Started process (PID=7365) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:56:16.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:56:16.177+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:56:16.176+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:56:16.197+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:56:16.226+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:56:16.225+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:56:16.256+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:56:16.255+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:56:16.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.121 seconds
[2024-03-23T18:56:46.392+0000] {processor.py:161} INFO - Started process (PID=7381) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:56:46.394+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:56:46.396+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:56:46.396+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:56:46.416+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:56:46.446+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:56:46.445+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:56:46.478+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:56:46.478+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:56:46.506+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.117 seconds
[2024-03-23T18:57:16.610+0000] {processor.py:161} INFO - Started process (PID=7397) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:57:16.611+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:57:16.614+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:57:16.613+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:57:16.633+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:57:16.665+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:57:16.664+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:57:16.698+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:57:16.697+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:57:16.731+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.125 seconds
[2024-03-23T18:57:46.833+0000] {processor.py:161} INFO - Started process (PID=7413) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:57:46.834+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:57:46.838+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:57:46.837+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:57:46.856+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:57:46.889+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:57:46.889+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:57:46.921+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:57:46.920+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:57:46.957+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.128 seconds
[2024-03-23T18:58:17.050+0000] {processor.py:161} INFO - Started process (PID=7429) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:58:17.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:58:17.055+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:58:17.054+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:58:17.075+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:58:17.109+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:58:17.109+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:58:17.143+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:58:17.142+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:58:17.171+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.124 seconds
[2024-03-23T18:58:47.255+0000] {processor.py:161} INFO - Started process (PID=7445) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:58:47.257+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:58:47.260+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:58:47.259+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:58:47.277+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:58:47.309+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:58:47.308+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:58:47.340+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:58:47.340+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:58:47.374+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.123 seconds
[2024-03-23T18:59:17.477+0000] {processor.py:161} INFO - Started process (PID=7461) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:59:17.478+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:59:17.482+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:59:17.481+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:59:17.500+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:59:17.529+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:59:17.528+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:59:17.560+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:59:17.560+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:59:17.589+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.115 seconds
[2024-03-23T18:59:47.688+0000] {processor.py:161} INFO - Started process (PID=7477) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T18:59:47.689+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T18:59:47.692+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:59:47.692+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T18:59:47.710+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T18:59:47.739+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:59:47.738+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T18:59:47.771+0000] {logging_mixin.py:188} INFO - [2024-03-23T18:59:47.770+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T18:59:47.804+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.119 seconds
[2024-03-23T19:00:17.908+0000] {processor.py:161} INFO - Started process (PID=7493) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:00:17.909+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:00:17.912+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:00:17.911+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:00:17.931+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:00:17.958+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:00:17.958+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:00:17.987+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:00:17.987+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:00:18.019+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.115 seconds
[2024-03-23T19:00:48.127+0000] {processor.py:161} INFO - Started process (PID=7509) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:00:48.129+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:00:48.134+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:00:48.133+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:00:48.153+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:00:48.183+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:00:48.182+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:00:48.213+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:00:48.212+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:00:48.241+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.117 seconds
[2024-03-23T19:01:18.420+0000] {processor.py:161} INFO - Started process (PID=7525) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:01:18.421+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:01:18.425+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:01:18.424+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:01:18.442+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:01:18.470+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:01:18.469+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:01:18.500+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:01:18.500+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:01:18.533+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.116 seconds
[2024-03-23T19:01:48.610+0000] {processor.py:161} INFO - Started process (PID=7541) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:01:48.612+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:01:48.615+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:01:48.615+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:01:48.632+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:01:48.662+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:01:48.661+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:01:48.689+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:01:48.688+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:01:48.722+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.115 seconds
[2024-03-23T19:02:18.821+0000] {processor.py:161} INFO - Started process (PID=7557) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:02:18.823+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:02:18.826+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:02:18.825+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:02:18.843+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:02:18.871+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:02:18.870+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:02:18.906+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:02:18.906+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:02:18.930+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.114 seconds
[2024-03-23T19:02:49.020+0000] {processor.py:161} INFO - Started process (PID=7573) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:02:49.021+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:02:49.025+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:02:49.024+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:02:49.045+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:02:49.073+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:02:49.072+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:02:49.103+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:02:49.103+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:02:49.136+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.119 seconds
[2024-03-23T19:03:19.243+0000] {processor.py:161} INFO - Started process (PID=7589) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:03:19.245+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:03:19.251+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:03:19.250+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:03:19.272+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:03:19.299+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:03:19.298+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:03:19.329+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:03:19.329+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:03:19.354+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.118 seconds
[2024-03-23T19:03:49.460+0000] {processor.py:161} INFO - Started process (PID=7605) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:03:49.463+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:03:49.469+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:03:49.468+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:03:49.490+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:03:49.524+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:03:49.523+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:03:49.554+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:03:49.553+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:03:49.591+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.139 seconds
[2024-03-23T19:04:19.683+0000] {processor.py:161} INFO - Started process (PID=7621) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:04:19.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:04:19.688+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:04:19.688+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:04:19.710+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:04:19.749+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:04:19.749+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:04:19.790+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:04:19.789+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:04:19.828+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.151 seconds
[2024-03-23T19:04:49.887+0000] {processor.py:161} INFO - Started process (PID=7637) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:04:49.888+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:04:49.891+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:04:49.891+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:04:49.910+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:04:49.940+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:04:49.939+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:04:49.971+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:04:49.970+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:04:50.003+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.119 seconds
[2024-03-23T19:05:20.083+0000] {processor.py:161} INFO - Started process (PID=7652) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:05:20.085+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:05:20.088+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:05:20.088+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:05:20.105+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:05:20.136+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:05:20.135+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:05:20.167+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:05:20.166+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:05:20.199+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.120 seconds
[2024-03-23T19:05:50.288+0000] {processor.py:161} INFO - Started process (PID=7668) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:05:50.289+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:05:50.292+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:05:50.291+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:05:50.308+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:05:50.339+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:05:50.338+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:05:50.372+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:05:50.372+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:05:50.408+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.124 seconds
[2024-03-23T19:06:20.574+0000] {processor.py:161} INFO - Started process (PID=7684) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:06:20.575+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:06:20.578+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:06:20.577+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:06:20.595+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:06:20.624+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:06:20.623+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:06:20.656+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:06:20.655+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:06:20.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.118 seconds
[2024-03-23T19:06:50.755+0000] {processor.py:161} INFO - Started process (PID=7700) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:06:50.756+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:06:50.759+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:06:50.759+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:06:50.777+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:06:50.812+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:06:50.811+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:06:50.855+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:06:50.855+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:06:50.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.138 seconds
[2024-03-23T19:07:20.957+0000] {processor.py:161} INFO - Started process (PID=7716) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:07:20.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:07:20.961+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:07:20.961+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:07:20.980+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:07:21.008+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:07:21.008+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:07:21.039+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:07:21.039+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:07:21.066+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.116 seconds
[2024-03-23T19:07:51.146+0000] {processor.py:161} INFO - Started process (PID=7732) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:07:51.147+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:07:51.150+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:07:51.149+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:07:51.169+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:07:51.199+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:07:51.198+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:07:51.229+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:07:51.229+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:07:51.254+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.112 seconds
[2024-03-23T19:08:21.354+0000] {processor.py:161} INFO - Started process (PID=7748) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:08:21.355+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:08:21.358+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:08:21.357+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:08:21.374+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:08:21.402+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:08:21.401+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:08:21.433+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:08:21.433+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:08:21.467+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.117 seconds
[2024-03-23T19:08:51.559+0000] {processor.py:161} INFO - Started process (PID=7764) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:08:51.560+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:08:51.565+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:08:51.564+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:08:51.582+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:08:51.609+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:08:51.608+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:08:51.643+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:08:51.642+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:08:51.679+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.124 seconds
[2024-03-23T19:09:21.769+0000] {processor.py:161} INFO - Started process (PID=7780) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:09:21.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:09:21.774+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:09:21.773+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:09:21.790+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:09:21.825+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:09:21.825+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:09:21.862+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:09:21.861+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:09:21.884+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.119 seconds
[2024-03-23T19:09:51.970+0000] {processor.py:161} INFO - Started process (PID=7802) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:09:51.971+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:09:51.975+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:09:51.974+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:09:51.991+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:09:52.025+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:09:52.024+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:09:52.054+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:09:52.054+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:09:52.077+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.111 seconds
[2024-03-23T19:10:22.175+0000] {processor.py:161} INFO - Started process (PID=7818) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:10:22.176+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:10:22.179+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:10:22.179+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:10:22.198+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:10:22.226+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:10:22.225+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:10:22.255+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:10:22.255+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:10:22.291+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.120 seconds
[2024-03-23T19:10:52.401+0000] {processor.py:161} INFO - Started process (PID=7834) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:10:52.402+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:10:52.406+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:10:52.405+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:10:52.427+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:10:52.457+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:10:52.456+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:10:52.489+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:10:52.489+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:10:52.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.128 seconds
[2024-03-23T19:11:22.638+0000] {processor.py:161} INFO - Started process (PID=7850) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:11:22.640+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:11:22.645+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:11:22.643+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:11:22.670+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:11:22.704+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:11:22.703+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:11:22.737+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:11:22.736+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:11:22.761+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.127 seconds
[2024-03-23T19:11:52.827+0000] {processor.py:161} INFO - Started process (PID=7866) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:11:52.829+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:11:52.832+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:11:52.831+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:11:52.849+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:11:52.881+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:11:52.880+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:11:52.912+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:11:52.912+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:11:52.933+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.110 seconds
[2024-03-23T19:12:23.087+0000] {processor.py:161} INFO - Started process (PID=7882) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:12:23.088+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:12:23.092+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:12:23.092+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:12:23.109+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:12:23.138+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:12:23.138+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:12:23.166+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:12:23.166+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:12:23.201+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.117 seconds
[2024-03-23T19:12:53.263+0000] {processor.py:161} INFO - Started process (PID=7898) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:12:53.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:12:53.268+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:12:53.267+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:12:53.286+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:12:53.312+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:12:53.311+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:12:53.342+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:12:53.341+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:12:53.373+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.114 seconds
[2024-03-23T19:13:23.473+0000] {processor.py:161} INFO - Started process (PID=7914) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:13:23.475+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:13:23.481+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:13:23.480+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:13:23.498+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:13:23.525+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:13:23.524+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:13:23.554+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:13:23.554+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:13:23.585+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.119 seconds
[2024-03-23T19:13:53.694+0000] {processor.py:161} INFO - Started process (PID=7930) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:13:53.696+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:13:53.699+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:13:53.698+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:13:53.716+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:13:53.744+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:13:53.743+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:13:53.774+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:13:53.774+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:13:53.809+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.118 seconds
[2024-03-23T19:14:23.896+0000] {processor.py:161} INFO - Started process (PID=7946) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:14:23.897+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:14:23.902+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:14:23.901+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:14:23.920+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:14:23.947+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:14:23.946+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:14:23.996+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:14:23.996+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:14:24.031+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.138 seconds
[2024-03-23T19:14:54.127+0000] {processor.py:161} INFO - Started process (PID=7962) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:14:54.129+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:14:54.136+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:14:54.135+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:14:54.174+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:14:54.213+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:14:54.212+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:14:54.244+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:14:54.243+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:14:54.269+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.149 seconds
[2024-03-23T19:15:24.485+0000] {processor.py:161} INFO - Started process (PID=7978) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:15:24.487+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:15:24.493+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:15:24.492+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:15:24.527+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:15:24.592+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:15:24.591+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:15:24.638+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:15:24.637+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:15:24.662+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.185 seconds
[2024-03-23T19:15:54.714+0000] {processor.py:161} INFO - Started process (PID=7994) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:15:54.717+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:15:54.723+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:15:54.721+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:15:54.756+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:15:54.804+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:15:54.803+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:15:54.864+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:15:54.863+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:15:54.898+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.191 seconds
[2024-03-23T19:16:25.017+0000] {processor.py:161} INFO - Started process (PID=8010) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:16:25.020+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:16:25.025+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:16:25.024+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:16:25.055+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:16:25.107+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:16:25.107+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:16:25.134+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:16:25.134+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:16:25.166+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.157 seconds
[2024-03-23T19:16:55.378+0000] {processor.py:161} INFO - Started process (PID=8026) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:16:55.380+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:16:55.386+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:16:55.385+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:16:55.426+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:16:55.458+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:16:55.457+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:16:55.490+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:16:55.490+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:16:55.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.153 seconds
[2024-03-23T19:17:25.605+0000] {processor.py:161} INFO - Started process (PID=8042) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:17:25.608+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:17:25.614+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:17:25.612+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:17:25.648+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:17:25.684+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:17:25.684+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:17:25.716+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:17:25.716+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:17:25.752+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.154 seconds
[2024-03-23T19:17:55.866+0000] {processor.py:161} INFO - Started process (PID=8058) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:17:55.868+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:17:55.873+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:17:55.872+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:17:55.907+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:17:55.963+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:17:55.962+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:17:56.021+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:17:56.020+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:17:56.061+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.201 seconds
[2024-03-23T19:18:26.154+0000] {processor.py:161} INFO - Started process (PID=8074) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:18:26.156+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:18:26.163+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:18:26.162+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:18:26.195+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:18:26.249+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:18:26.248+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:18:26.283+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:18:26.282+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:18:26.316+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.169 seconds
[2024-03-23T19:18:56.419+0000] {processor.py:161} INFO - Started process (PID=8090) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:18:56.420+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:18:56.423+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:18:56.422+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:18:56.440+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:18:56.494+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:18:56.493+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:18:56.526+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:18:56.526+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:18:56.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.158 seconds
[2024-03-23T19:19:26.782+0000] {processor.py:161} INFO - Started process (PID=8106) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:19:26.784+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:19:26.790+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:19:26.788+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:19:26.827+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:19:26.867+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:19:26.866+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:19:26.896+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:19:26.896+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:19:26.927+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.152 seconds
[2024-03-23T19:19:57.008+0000] {processor.py:161} INFO - Started process (PID=8122) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:19:57.010+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:19:57.016+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:19:57.015+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:19:57.051+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:19:57.086+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:19:57.085+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:19:57.116+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:19:57.116+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:19:57.143+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.144 seconds
[2024-03-23T19:20:27.260+0000] {processor.py:161} INFO - Started process (PID=8138) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:20:27.262+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:20:27.268+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:20:27.267+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:20:27.304+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:20:27.357+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:20:27.356+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:20:27.386+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:20:27.385+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:20:27.417+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.160 seconds
[2024-03-23T19:20:57.548+0000] {processor.py:161} INFO - Started process (PID=8154) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:20:57.551+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:20:57.557+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:20:57.555+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:20:57.593+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:20:57.628+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:20:57.627+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:20:57.657+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:20:57.657+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:20:57.692+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.150 seconds
[2024-03-23T19:21:27.905+0000] {processor.py:161} INFO - Started process (PID=8170) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:21:27.907+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:21:27.913+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:21:27.912+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:21:27.949+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:21:28.008+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:21:28.007+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:21:28.037+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:21:28.037+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:21:28.070+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.171 seconds
[2024-03-23T19:21:58.121+0000] {processor.py:161} INFO - Started process (PID=8186) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:21:58.123+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:21:58.130+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:21:58.128+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:21:58.169+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:21:58.210+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:21:58.209+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:21:58.240+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:21:58.239+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:21:58.271+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.157 seconds
[2024-03-23T19:22:17.286+0000] {processor.py:161} INFO - Started process (PID=8191) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:22:17.287+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:22:17.292+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:22:17.291+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:22:17.321+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:22:17.316+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 60, in <module>
    transform_data = PythonOperator(
                     ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/python.py", line 184, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_csv_file' has already been added to the DAG
[2024-03-23T19:22:17.322+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:22:17.354+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.073 seconds
[2024-03-23T19:22:25.358+0000] {processor.py:161} INFO - Started process (PID=8207) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:22:25.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:22:25.363+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:22:25.362+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:22:25.390+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:22:25.385+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 60, in <module>
    transform_data = PythonOperator(
                     ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/python.py", line 184, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_csv_file' has already been added to the DAG
[2024-03-23T19:22:25.392+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:22:25.426+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.072 seconds
[2024-03-23T19:22:50.600+0000] {processor.py:161} INFO - Started process (PID=8217) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:22:50.602+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:22:50.607+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:22:50.606+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:22:50.629+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:22:50.626+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/pipeline.py", line 60, in <module>
    transform_data = PythonOperator(
                     ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/python.py", line 184, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_csv_file' has already been added to the DAG
[2024-03-23T19:22:50.630+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:22:50.661+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.067 seconds
[2024-03-23T19:23:08.747+0000] {processor.py:161} INFO - Started process (PID=8228) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:23:08.748+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:23:08.753+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:23:08.752+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:23:08.791+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:23:08.938+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:23:08.937+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:23:08.966+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:23:08.965+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:23:09.001+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.262 seconds
[2024-03-23T19:23:18.844+0000] {processor.py:161} INFO - Started process (PID=8233) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:23:18.846+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:23:18.852+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:23:18.851+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:23:18.895+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:23:18.913+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:23:18.912+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:23:18.940+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:23:18.940+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:23:18.970+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.135 seconds
[2024-03-23T19:23:32.004+0000] {processor.py:161} INFO - Started process (PID=8249) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:23:32.005+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:23:32.009+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:23:32.008+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:23:32.035+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:23:32.050+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:23:32.050+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:23:32.080+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:23:32.079+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:23:32.106+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.107 seconds
[2024-03-23T19:23:52.249+0000] {processor.py:161} INFO - Started process (PID=8259) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:23:52.250+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:23:52.254+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:23:52.254+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:23:52.292+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:23:52.435+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:23:52.434+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:23:52.467+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:23:52.467+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:23:52.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.273 seconds
[2024-03-23T19:24:23.422+0000] {processor.py:161} INFO - Started process (PID=8275) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:24:23.423+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:24:23.426+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:24:23.425+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:24:23.444+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:24:23.474+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:24:23.473+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:24:23.502+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:24:23.502+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:24:23.526+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.107 seconds
[2024-03-23T19:24:38.589+0000] {processor.py:161} INFO - Started process (PID=8286) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:24:38.590+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:24:38.593+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:24:38.592+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:24:38.618+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:24:38.777+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:24:38.776+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:24:38.819+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:24:38.818+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:24:38.860+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.275 seconds
[2024-03-23T19:25:09.823+0000] {processor.py:161} INFO - Started process (PID=8302) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:25:09.826+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:25:09.833+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:25:09.832+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:25:09.862+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:25:09.888+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:25:09.887+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:25:09.926+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:25:09.926+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:25:09.952+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.136 seconds
[2024-03-23T19:25:40.060+0000] {processor.py:161} INFO - Started process (PID=8318) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:25:40.062+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:25:40.067+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:25:40.066+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:25:40.094+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:25:40.121+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:25:40.120+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:25:40.149+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:25:40.149+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:25:40.174+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.120 seconds
[2024-03-23T19:26:10.317+0000] {processor.py:161} INFO - Started process (PID=8334) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:26:10.318+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:26:10.322+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:26:10.321+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:26:10.343+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:26:10.400+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:26:10.399+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:26:10.429+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:26:10.429+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:26:10.463+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.150 seconds
[2024-03-23T19:26:40.525+0000] {processor.py:161} INFO - Started process (PID=8350) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:26:40.526+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:26:40.529+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:26:40.528+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:26:40.546+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:26:40.572+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:26:40.572+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:26:40.601+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:26:40.601+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:26:40.627+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.107 seconds
[2024-03-23T19:27:10.753+0000] {processor.py:161} INFO - Started process (PID=8366) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:27:10.755+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:27:10.761+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:27:10.759+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:27:10.782+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:27:10.809+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:27:10.809+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:27:10.836+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:27:10.836+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:27:10.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.121 seconds
[2024-03-23T19:27:40.992+0000] {processor.py:161} INFO - Started process (PID=8382) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:27:40.993+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:27:40.996+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:27:40.995+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:27:41.020+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:27:41.054+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:27:41.053+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:27:41.103+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:27:41.103+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:27:41.137+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.150 seconds
[2024-03-23T19:28:11.237+0000] {processor.py:161} INFO - Started process (PID=8398) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:28:11.238+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:28:11.243+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:28:11.242+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:28:11.278+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:28:11.330+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:28:11.330+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:28:11.356+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:28:11.356+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:28:11.389+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.156 seconds
[2024-03-23T19:28:32.473+0000] {processor.py:161} INFO - Started process (PID=8414) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:28:32.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:28:32.478+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:28:32.477+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:28:32.518+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:28:32.796+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:28:32.794+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:28:32.844+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:28:32.844+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:28:32.891+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.423 seconds
[2024-03-23T19:29:03.661+0000] {processor.py:161} INFO - Started process (PID=8430) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:29:03.662+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:29:03.665+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:29:03.664+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:29:03.683+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:29:03.709+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:29:03.708+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:29:03.738+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:29:03.738+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:29:03.775+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.118 seconds
[2024-03-23T19:29:33.882+0000] {processor.py:161} INFO - Started process (PID=8470) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:29:33.883+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:29:33.886+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:29:33.885+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:29:33.908+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:29:33.938+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:29:33.938+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:29:33.968+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:29:33.968+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:29:34.007+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.129 seconds
[2024-03-23T19:30:04.193+0000] {processor.py:161} INFO - Started process (PID=8486) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:30:04.194+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:30:04.197+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:30:04.197+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:30:04.215+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:30:04.245+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:30:04.244+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:30:04.275+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:30:04.275+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:30:04.306+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.117 seconds
[2024-03-23T19:30:34.397+0000] {processor.py:161} INFO - Started process (PID=8502) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:30:34.399+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:30:34.404+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:30:34.403+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:30:34.426+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:30:34.460+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:30:34.459+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:30:34.494+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:30:34.494+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:30:34.521+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.131 seconds
[2024-03-23T19:31:04.693+0000] {processor.py:161} INFO - Started process (PID=8518) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:31:04.694+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:31:04.698+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:31:04.697+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:31:04.723+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:31:04.754+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:31:04.753+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:31:04.801+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:31:04.801+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:31:04.836+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.147 seconds
[2024-03-23T19:31:34.886+0000] {processor.py:161} INFO - Started process (PID=8534) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:31:34.888+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:31:34.891+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:31:34.890+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:31:34.912+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:31:34.969+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:31:34.968+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:31:35.006+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:31:35.006+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:31:35.050+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.167 seconds
[2024-03-23T19:32:05.119+0000] {processor.py:161} INFO - Started process (PID=8550) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:32:05.121+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:32:05.124+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:32:05.123+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:32:05.145+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:32:05.178+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:32:05.177+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:32:05.213+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:32:05.213+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:32:05.249+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.133 seconds
[2024-03-23T19:32:35.393+0000] {processor.py:161} INFO - Started process (PID=8571) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:32:35.394+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:32:35.397+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:32:35.396+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:32:35.417+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:32:35.449+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:32:35.448+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:32:35.479+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:32:35.479+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:32:35.513+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.124 seconds
[2024-03-23T19:32:51.543+0000] {processor.py:161} INFO - Started process (PID=8593) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:32:51.547+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:32:51.552+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:32:51.551+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:32:51.603+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:32:51.804+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:32:51.803+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:32:51.840+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:32:51.839+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:32:51.885+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.348 seconds
[2024-03-23T19:33:22.811+0000] {processor.py:161} INFO - Started process (PID=8636) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:33:22.813+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:33:22.817+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:33:22.816+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:33:22.841+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:33:22.876+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:33:22.875+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:33:22.909+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:33:22.909+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:33:22.947+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.144 seconds
[2024-03-23T19:33:37.939+0000] {processor.py:161} INFO - Started process (PID=8647) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:33:37.940+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:33:37.943+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:33:37.943+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:33:37.967+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:33:37.992+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:33:37.991+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:33:38.020+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:33:38.019+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:33:38.049+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.114 seconds
[2024-03-23T19:33:43.101+0000] {processor.py:161} INFO - Started process (PID=8652) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:33:43.102+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:33:43.105+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:33:43.104+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:33:43.130+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:33:43.163+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:33:43.163+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:33:43.195+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:33:43.194+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:33:43.222+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.126 seconds
[2024-03-23T19:34:13.298+0000] {processor.py:161} INFO - Started process (PID=8680) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:34:13.300+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:34:13.306+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:34:13.305+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:34:13.343+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:34:13.403+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:34:13.402+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:34:13.435+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:34:13.435+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:34:13.472+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.181 seconds
[2024-03-23T19:34:43.532+0000] {processor.py:161} INFO - Started process (PID=8696) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:34:43.533+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:34:43.537+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:34:43.536+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:34:43.558+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:34:43.585+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:34:43.584+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:34:43.613+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:34:43.613+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:34:43.634+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.106 seconds
[2024-03-23T19:35:13.839+0000] {processor.py:161} INFO - Started process (PID=8712) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:35:13.840+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:35:13.843+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:35:13.842+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:35:13.864+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:35:13.895+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:35:13.894+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:35:13.934+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:35:13.934+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:35:13.971+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.136 seconds
[2024-03-23T19:35:44.023+0000] {processor.py:161} INFO - Started process (PID=8728) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:35:44.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:35:44.028+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:35:44.027+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:35:44.047+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:35:44.090+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:35:44.089+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:35:44.132+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:35:44.131+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:35:44.169+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.149 seconds
[2024-03-23T19:36:14.277+0000] {processor.py:161} INFO - Started process (PID=8744) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:36:14.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:36:14.283+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:36:14.282+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:36:14.322+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:36:14.356+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:36:14.355+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:36:14.401+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:36:14.400+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:36:14.441+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.171 seconds
[2024-03-23T19:36:44.510+0000] {processor.py:161} INFO - Started process (PID=8760) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:36:44.512+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:36:44.515+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:36:44.515+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:36:44.554+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:36:44.613+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:36:44.612+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:36:44.667+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:36:44.666+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:36:44.706+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.204 seconds
[2024-03-23T19:37:14.751+0000] {processor.py:161} INFO - Started process (PID=8776) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:37:14.753+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:37:14.758+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:37:14.757+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:37:14.780+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:37:14.822+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:37:14.821+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:37:14.860+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:37:14.860+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:37:14.900+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.155 seconds
[2024-03-23T19:37:44.952+0000] {processor.py:161} INFO - Started process (PID=8792) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:37:44.957+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:37:44.964+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:37:44.963+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:37:44.994+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:37:45.033+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:37:45.032+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:37:45.074+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:37:45.073+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:37:45.107+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.161 seconds
[2024-03-23T19:38:15.171+0000] {processor.py:161} INFO - Started process (PID=8808) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:38:15.172+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:38:15.175+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:38:15.175+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:38:15.205+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:38:15.235+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:38:15.233+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:38:15.264+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:38:15.263+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:38:15.295+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.128 seconds
[2024-03-23T19:38:45.404+0000] {processor.py:161} INFO - Started process (PID=8836) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:38:45.405+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:38:45.409+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:38:45.408+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:38:45.429+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:38:45.459+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:38:45.459+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:38:45.491+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:38:45.490+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:38:45.526+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.129 seconds
[2024-03-23T19:39:15.616+0000] {processor.py:161} INFO - Started process (PID=8852) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:39:15.617+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:39:15.620+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:39:15.619+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:39:15.643+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:39:15.675+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:39:15.675+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:39:15.706+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:39:15.706+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:39:15.742+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.130 seconds
[2024-03-23T19:39:45.922+0000] {processor.py:161} INFO - Started process (PID=8873) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:39:45.924+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:39:45.929+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:39:45.928+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:39:45.966+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:39:46.029+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:39:46.027+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:39:46.059+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:39:46.059+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:39:46.098+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.182 seconds
[2024-03-23T19:40:13.113+0000] {processor.py:161} INFO - Started process (PID=8889) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:40:13.115+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:40:13.119+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:40:13.118+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:40:13.148+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:40:13.310+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:40:13.309+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:40:13.336+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:40:13.336+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:40:13.372+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.263 seconds
[2024-03-23T19:40:44.363+0000] {processor.py:161} INFO - Started process (PID=8905) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:40:44.365+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:40:44.371+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:40:44.369+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:40:44.405+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:40:44.458+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:40:44.457+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:40:44.488+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:40:44.487+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:40:44.528+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.171 seconds
[2024-03-23T19:41:14.625+0000] {processor.py:161} INFO - Started process (PID=8921) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:41:14.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:41:14.631+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:41:14.631+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:41:14.655+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:41:14.685+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:41:14.684+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:41:14.713+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:41:14.713+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:41:14.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.119 seconds
[2024-03-23T19:41:28.768+0000] {processor.py:161} INFO - Started process (PID=8932) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:41:28.769+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:41:28.772+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:41:28.772+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:41:28.797+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:41:28.834+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:41:28.833+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:41:28.865+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:41:28.865+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:41:28.900+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.136 seconds
[2024-03-23T19:41:59.021+0000] {processor.py:161} INFO - Started process (PID=8960) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:41:59.023+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:41:59.028+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:41:59.026+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:41:59.054+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:41:59.090+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:41:59.089+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:41:59.123+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:41:59.123+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:41:59.161+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.144 seconds
[2024-03-23T19:42:29.255+0000] {processor.py:161} INFO - Started process (PID=8976) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:42:29.257+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:42:29.263+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:42:29.262+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:42:29.282+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:42:29.318+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:42:29.317+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:42:29.378+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:42:29.377+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:42:29.414+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.167 seconds
[2024-03-23T19:42:59.475+0000] {processor.py:161} INFO - Started process (PID=8992) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:42:59.477+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:42:59.482+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:42:59.481+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:42:59.503+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:42:59.554+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:42:59.553+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:42:59.586+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:42:59.586+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:42:59.616+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.145 seconds
[2024-03-23T19:43:29.718+0000] {processor.py:161} INFO - Started process (PID=9017) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:43:29.719+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:43:29.723+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:43:29.722+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:43:29.750+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:43:29.786+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:43:29.785+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:43:29.815+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:43:29.815+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:43:29.837+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.124 seconds
[2024-03-23T19:43:46.859+0000] {processor.py:161} INFO - Started process (PID=9027) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:43:46.860+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:43:46.863+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:43:46.862+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:43:46.888+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:43:47.056+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:43:47.055+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:43:47.097+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:43:47.096+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:43:47.151+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.297 seconds
[2024-03-23T19:44:18.113+0000] {processor.py:161} INFO - Started process (PID=9077) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:44:18.115+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:44:18.119+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:44:18.118+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:44:18.147+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:44:18.184+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:44:18.184+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:44:18.219+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:44:18.219+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:44:18.256+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.148 seconds
[2024-03-23T19:44:48.442+0000] {processor.py:161} INFO - Started process (PID=9093) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:44:48.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:44:48.447+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:44:48.446+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:44:48.490+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:44:48.684+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:44:48.683+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:44:48.714+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:44:48.713+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:44:48.740+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.301 seconds
[2024-03-23T19:45:19.634+0000] {processor.py:161} INFO - Started process (PID=9109) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:45:19.635+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:45:19.638+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:45:19.638+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:45:19.662+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:45:19.694+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:45:19.693+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:45:19.728+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:45:19.728+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:45:19.763+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.133 seconds
[2024-03-23T19:45:49.938+0000] {processor.py:161} INFO - Started process (PID=9125) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:45:49.940+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:45:49.947+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:45:49.945+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:45:49.973+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:45:50.003+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:45:50.002+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:45:50.036+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:45:50.035+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:45:50.065+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.133 seconds
[2024-03-23T19:46:20.138+0000] {processor.py:161} INFO - Started process (PID=9144) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:46:20.139+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:46:20.143+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:46:20.142+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:46:20.161+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:46:20.191+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:46:20.190+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:46:20.220+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:46:20.220+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:46:20.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.121 seconds
[2024-03-23T19:46:50.370+0000] {processor.py:161} INFO - Started process (PID=9160) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:46:50.372+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:46:50.380+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:46:50.379+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:46:50.415+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:46:50.469+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:46:50.468+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:46:50.501+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:46:50.501+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:46:50.528+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.166 seconds
[2024-03-23T19:47:20.601+0000] {processor.py:161} INFO - Started process (PID=9204) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:47:20.604+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:47:20.613+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:47:20.612+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:47:20.645+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:47:20.685+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:47:20.684+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:47:20.723+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:47:20.723+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:47:20.760+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.168 seconds
[2024-03-23T19:47:50.845+0000] {processor.py:161} INFO - Started process (PID=9262) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:47:50.847+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:47:50.855+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:47:50.854+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:47:50.885+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:47:50.911+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:47:50.911+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:47:50.941+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:47:50.940+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:47:50.973+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.137 seconds
[2024-03-23T19:48:21.071+0000] {processor.py:161} INFO - Started process (PID=9297) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:48:21.074+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:48:21.080+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:48:21.080+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:48:21.103+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:48:21.146+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:48:21.145+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:48:21.181+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:48:21.181+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:48:21.220+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.154 seconds
[2024-03-23T19:48:51.296+0000] {processor.py:161} INFO - Started process (PID=9328) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:48:51.297+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:48:51.302+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:48:51.301+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:48:51.322+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:48:51.358+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:48:51.358+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:48:51.393+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:48:51.393+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:48:51.433+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.144 seconds
[2024-03-23T19:49:21.634+0000] {processor.py:161} INFO - Started process (PID=9344) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:49:21.637+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:49:21.641+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:49:21.641+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:49:21.659+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:49:21.702+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:49:21.701+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:49:21.733+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:49:21.732+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:49:21.768+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.142 seconds
[2024-03-23T19:49:51.826+0000] {processor.py:161} INFO - Started process (PID=9360) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:49:51.828+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:49:51.834+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:49:51.832+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:49:51.865+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:49:51.900+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:49:51.900+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:49:51.934+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:49:51.933+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:49:51.970+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.149 seconds
[2024-03-23T19:50:22.077+0000] {processor.py:161} INFO - Started process (PID=9376) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:50:22.078+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:50:22.082+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:50:22.081+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:50:22.104+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:50:22.145+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:50:22.144+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:50:22.178+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:50:22.178+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:50:22.204+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.132 seconds
[2024-03-23T19:50:52.318+0000] {processor.py:161} INFO - Started process (PID=9392) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:50:52.319+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:50:52.324+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:50:52.323+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:50:52.347+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:50:52.389+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:50:52.388+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:50:52.427+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:50:52.426+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:50:52.465+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.153 seconds
[2024-03-23T19:51:22.657+0000] {processor.py:161} INFO - Started process (PID=9408) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:51:22.659+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:51:22.663+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:51:22.662+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:51:22.696+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:51:22.731+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:51:22.730+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:51:22.763+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:51:22.763+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:51:22.797+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.145 seconds
[2024-03-23T19:51:52.859+0000] {processor.py:161} INFO - Started process (PID=9424) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:51:52.862+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:51:52.867+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:51:52.866+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:51:52.908+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:51:52.946+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:51:52.945+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:51:52.979+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:51:52.978+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:51:53.007+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.155 seconds
[2024-03-23T19:52:23.096+0000] {processor.py:161} INFO - Started process (PID=9440) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:52:23.097+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:52:23.100+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:52:23.100+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:52:23.121+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:52:23.157+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:52:23.156+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:52:23.191+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:52:23.190+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:52:23.224+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.132 seconds
[2024-03-23T19:52:53.335+0000] {processor.py:161} INFO - Started process (PID=9456) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:52:53.337+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:52:53.341+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:52:53.340+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:52:53.362+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:52:53.396+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:52:53.395+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:52:53.433+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:52:53.433+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:52:53.475+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.144 seconds
[2024-03-23T19:53:23.579+0000] {processor.py:161} INFO - Started process (PID=9472) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:53:23.581+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:53:23.584+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:53:23.583+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:53:23.606+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:53:23.649+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:53:23.648+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:53:23.689+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:53:23.689+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:53:23.723+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.147 seconds
[2024-03-23T19:53:53.812+0000] {processor.py:161} INFO - Started process (PID=9488) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:53:53.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:53:53.817+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:53:53.816+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:53:53.842+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:53:53.881+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:53:53.880+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:53:53.914+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:53:53.914+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:53:53.942+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.134 seconds
[2024-03-23T19:54:24.042+0000] {processor.py:161} INFO - Started process (PID=9504) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:54:24.043+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:54:24.047+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:54:24.046+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:54:24.075+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:54:24.115+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:54:24.114+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:54:24.149+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:54:24.149+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:54:24.189+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.151 seconds
[2024-03-23T19:54:54.278+0000] {processor.py:161} INFO - Started process (PID=9520) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:54:54.280+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:54:54.284+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:54:54.282+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:54:54.308+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:54:54.349+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:54:54.348+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:54:54.389+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:54:54.389+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:54:54.436+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.162 seconds
[2024-03-23T19:55:24.539+0000] {processor.py:161} INFO - Started process (PID=9536) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:55:24.541+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:55:24.545+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:55:24.544+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:55:24.569+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:55:24.609+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:55:24.608+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:55:24.650+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:55:24.650+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:55:24.681+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.147 seconds
[2024-03-23T19:55:54.794+0000] {processor.py:161} INFO - Started process (PID=9552) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:55:54.796+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:55:54.800+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:55:54.799+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:55:54.828+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:55:54.866+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:55:54.866+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:55:54.903+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:55:54.903+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:55:54.931+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.141 seconds
[2024-03-23T19:56:25.037+0000] {processor.py:161} INFO - Started process (PID=9568) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:56:25.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:56:25.042+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:56:25.041+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:56:25.072+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:56:25.098+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:56:25.097+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:56:25.125+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:56:25.125+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:56:25.148+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.115 seconds
[2024-03-23T19:56:55.335+0000] {processor.py:161} INFO - Started process (PID=9584) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:56:55.336+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:56:55.340+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:56:55.339+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:56:55.359+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:56:55.390+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:56:55.389+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:56:55.421+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:56:55.421+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:56:55.454+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.123 seconds
[2024-03-23T19:57:25.557+0000] {processor.py:161} INFO - Started process (PID=9600) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:57:25.558+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:57:25.562+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:57:25.561+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:57:25.577+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:57:25.609+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:57:25.608+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:57:25.640+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:57:25.640+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:57:25.672+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.118 seconds
[2024-03-23T19:57:55.783+0000] {processor.py:161} INFO - Started process (PID=9616) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:57:55.785+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:57:55.791+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:57:55.790+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:57:55.815+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:57:55.854+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:57:55.853+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:57:55.893+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:57:55.893+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:57:55.935+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.158 seconds
[2024-03-23T19:58:26.038+0000] {processor.py:161} INFO - Started process (PID=9632) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:58:26.040+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:58:26.047+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:58:26.046+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:58:26.088+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:58:26.137+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:58:26.136+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:58:26.170+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:58:26.169+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:58:26.200+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.170 seconds
[2024-03-23T19:58:56.278+0000] {processor.py:161} INFO - Started process (PID=9648) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:58:56.280+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:58:56.286+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:58:56.284+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:58:56.309+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:58:56.346+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:58:56.345+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:58:56.388+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:58:56.387+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:58:56.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.154 seconds
[2024-03-23T19:59:26.517+0000] {processor.py:161} INFO - Started process (PID=9664) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:59:26.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:59:26.525+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:59:26.523+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:59:26.564+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:59:26.620+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:59:26.619+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:59:26.650+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:59:26.650+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-23 00:00:00+00:00, run_after=2024-03-24 00:00:00+00:00
[2024-03-23T19:59:26.677+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.168 seconds
[2024-03-23T19:59:56.746+0000] {processor.py:161} INFO - Started process (PID=9680) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T19:59:56.748+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T19:59:56.753+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:59:56.752+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T19:59:56.794+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T19:59:56.911+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:59:56.910+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T19:59:56.925+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:59:56.925+0000] {dag.py:3058} INFO - Creating ORM DAG for etl_to_bigquery
[2024-03-23T19:59:56.940+0000] {logging_mixin.py:188} INFO - [2024-03-23T19:59:56.939+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T19:59:56.973+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.235 seconds
[2024-03-23T20:00:27.065+0000] {processor.py:161} INFO - Started process (PID=9696) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:00:27.067+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:00:27.073+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:00:27.071+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:00:27.094+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:00:27.142+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:00:27.141+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:00:27.196+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:00:27.195+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:00:27.228+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.171 seconds
[2024-03-23T20:00:57.318+0000] {processor.py:161} INFO - Started process (PID=9712) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:00:57.320+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:00:57.326+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:00:57.325+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:00:57.363+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:00:57.394+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:00:57.394+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:00:57.430+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:00:57.430+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:00:57.473+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.161 seconds
[2024-03-23T20:01:27.521+0000] {processor.py:161} INFO - Started process (PID=9728) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:01:27.522+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:01:27.525+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:01:27.524+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:01:27.543+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:01:27.583+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:01:27.582+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:01:27.621+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:01:27.621+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:01:27.662+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.145 seconds
[2024-03-23T20:01:57.737+0000] {processor.py:161} INFO - Started process (PID=9744) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:01:57.738+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:01:57.741+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:01:57.740+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:01:57.761+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:01:57.794+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:01:57.793+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:01:57.822+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:01:57.821+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:01:57.855+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.123 seconds
[2024-03-23T20:02:28.025+0000] {processor.py:161} INFO - Started process (PID=9760) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:02:28.026+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:02:28.031+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:02:28.029+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:02:28.055+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:02:28.082+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:02:28.082+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:02:28.129+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:02:28.129+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:02:28.165+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.144 seconds
[2024-03-23T20:02:58.210+0000] {processor.py:161} INFO - Started process (PID=9776) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:02:58.212+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:02:58.216+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:02:58.215+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:02:58.239+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:02:58.295+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:02:58.294+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:02:58.342+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:02:58.342+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:02:58.386+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.179 seconds
[2024-03-23T20:03:27.463+0000] {processor.py:161} INFO - Started process (PID=9792) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:03:27.466+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:03:27.472+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:03:27.470+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:03:27.501+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:03:27.526+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:03:27.526+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:03:27.555+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:03:27.555+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:03:27.582+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.126 seconds
[2024-03-23T20:03:57.761+0000] {processor.py:161} INFO - Started process (PID=9807) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:03:57.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:03:57.765+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:03:57.764+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:03:57.781+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:03:57.807+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:03:57.807+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:03:57.862+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:03:57.861+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:03:57.895+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.138 seconds
[2024-03-23T20:04:27.959+0000] {processor.py:161} INFO - Started process (PID=9823) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:04:27.960+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:04:27.963+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:04:27.963+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:04:27.984+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:04:28.020+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:04:28.019+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:04:28.054+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:04:28.054+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:04:28.088+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.133 seconds
[2024-03-23T20:04:58.264+0000] {processor.py:161} INFO - Started process (PID=9839) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:04:58.265+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:04:58.269+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:04:58.268+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:04:58.290+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:04:58.329+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:04:58.328+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:04:58.366+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:04:58.366+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:04:58.395+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.134 seconds
[2024-03-23T20:05:28.483+0000] {processor.py:161} INFO - Started process (PID=9855) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:05:28.485+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:05:28.489+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:05:28.489+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:05:28.523+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:05:28.575+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:05:28.574+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:05:28.601+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:05:28.601+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:05:28.638+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.162 seconds
[2024-03-23T20:05:58.832+0000] {processor.py:161} INFO - Started process (PID=9871) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:05:58.834+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:05:58.840+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:05:58.839+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:05:58.864+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:05:58.899+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:05:58.898+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:05:58.930+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:05:58.930+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:05:58.957+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.129 seconds
[2024-03-23T20:06:29.163+0000] {processor.py:161} INFO - Started process (PID=9887) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:06:29.165+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:06:29.172+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:06:29.170+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:06:29.206+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:06:29.263+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:06:29.261+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:06:29.312+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:06:29.311+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:06:29.347+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.190 seconds
[2024-03-23T20:06:59.435+0000] {processor.py:161} INFO - Started process (PID=9903) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:06:59.436+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:06:59.443+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:06:59.442+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:06:59.476+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:06:59.523+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:06:59.522+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:06:59.550+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:06:59.550+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:06:59.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.161 seconds
[2024-03-23T20:07:29.646+0000] {processor.py:161} INFO - Started process (PID=9919) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:07:29.648+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:07:29.654+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:07:29.652+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:07:29.684+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:07:29.729+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:07:29.728+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:07:29.768+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:07:29.767+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:07:29.811+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.171 seconds
[2024-03-23T20:07:59.859+0000] {processor.py:161} INFO - Started process (PID=9935) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:07:59.861+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:07:59.865+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:07:59.864+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:07:59.887+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:07:59.931+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:07:59.930+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:07:59.973+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:07:59.973+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:08:00.020+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.165 seconds
[2024-03-23T20:08:30.164+0000] {processor.py:161} INFO - Started process (PID=9951) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:08:30.166+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:08:30.173+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:08:30.172+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:08:30.214+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:08:30.270+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:08:30.269+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:08:30.320+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:08:30.320+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:08:30.374+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.216 seconds
[2024-03-23T20:09:00.471+0000] {processor.py:161} INFO - Started process (PID=9967) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:09:00.473+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:09:00.479+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:09:00.478+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:09:00.512+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:09:00.576+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:09:00.575+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:09:00.633+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:09:00.632+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:09:00.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.223 seconds
[2024-03-23T20:09:31.664+0000] {processor.py:161} INFO - Started process (PID=9989) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:09:31.666+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:09:31.672+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:09:31.670+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:09:31.696+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:09:31.723+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:09:31.722+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:09:31.751+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:09:31.751+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:09:31.794+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.136 seconds
[2024-03-23T20:10:01.996+0000] {processor.py:161} INFO - Started process (PID=10006) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:10:01.997+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:10:02.000+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:10:01.999+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:10:02.027+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:10:02.058+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:10:02.058+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:10:02.090+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:10:02.089+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:10:02.116+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.124 seconds
[2024-03-23T20:10:32.208+0000] {processor.py:161} INFO - Started process (PID=10022) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:10:32.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:10:32.212+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:10:32.211+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:10:32.232+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:10:32.260+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:10:32.260+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:10:32.289+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:10:32.288+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:10:32.330+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.126 seconds
[2024-03-23T20:11:02.511+0000] {processor.py:161} INFO - Started process (PID=10038) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:11:02.512+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:11:02.516+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:11:02.515+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:11:02.537+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:11:02.581+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:11:02.580+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:11:02.624+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:11:02.623+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:11:02.669+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.163 seconds
[2024-03-23T20:11:32.715+0000] {processor.py:161} INFO - Started process (PID=10055) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:11:32.718+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:11:32.722+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:11:32.721+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:11:32.745+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:11:32.781+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:11:32.780+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:11:32.821+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:11:32.821+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:11:32.858+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.148 seconds
[2024-03-23T20:12:02.923+0000] {processor.py:161} INFO - Started process (PID=10071) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:12:02.925+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:12:02.930+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:12:02.929+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:12:02.965+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:12:03.014+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:12:03.013+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:12:03.044+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:12:03.044+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:12:03.090+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.171 seconds
[2024-03-23T20:12:33.175+0000] {processor.py:161} INFO - Started process (PID=10087) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:12:33.176+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:12:33.180+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:12:33.179+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:12:33.219+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:12:33.276+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:12:33.276+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:12:33.306+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:12:33.306+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:12:33.347+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.176 seconds
[2024-03-23T20:13:03.460+0000] {processor.py:161} INFO - Started process (PID=10103) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:13:03.462+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:13:03.468+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:13:03.466+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:13:03.491+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:13:03.529+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:13:03.528+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:13:03.561+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:13:03.560+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:13:03.595+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.142 seconds
[2024-03-23T20:13:33.748+0000] {processor.py:161} INFO - Started process (PID=10119) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:13:33.751+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:13:33.757+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:13:33.756+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:13:33.790+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:13:33.843+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:13:33.841+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:13:33.884+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:13:33.883+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:13:33.919+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.178 seconds
[2024-03-23T20:14:03.987+0000] {processor.py:161} INFO - Started process (PID=10135) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:14:03.988+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:14:03.992+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:14:03.991+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:14:04.016+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:14:04.050+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:14:04.049+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:14:04.078+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:14:04.078+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:14:04.101+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.118 seconds
[2024-03-23T20:14:34.286+0000] {processor.py:161} INFO - Started process (PID=10151) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:14:34.288+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:14:34.295+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:14:34.294+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:14:34.333+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:14:34.382+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:14:34.382+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:14:34.414+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:14:34.414+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:14:34.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.165 seconds
[2024-03-23T20:15:04.514+0000] {processor.py:161} INFO - Started process (PID=10167) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:15:04.517+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:15:04.522+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:15:04.521+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:15:04.557+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:15:04.595+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:15:04.594+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:15:04.628+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:15:04.628+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:15:04.657+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.151 seconds
[2024-03-23T20:15:34.840+0000] {processor.py:161} INFO - Started process (PID=10183) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:15:34.841+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:15:34.844+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:15:34.843+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:15:34.862+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:15:34.892+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:15:34.891+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:15:34.925+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:15:34.925+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:15:34.965+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.129 seconds
[2024-03-23T20:16:05.082+0000] {processor.py:161} INFO - Started process (PID=10199) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:16:05.083+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:16:05.086+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:16:05.086+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:16:05.105+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:16:05.135+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:16:05.134+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:16:05.165+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:16:05.165+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:16:05.199+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.121 seconds
[2024-03-23T20:16:35.266+0000] {processor.py:161} INFO - Started process (PID=10216) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:16:35.267+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:16:35.271+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:16:35.270+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:16:35.304+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:16:35.337+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:16:35.337+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:16:35.372+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:16:35.372+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:16:35.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.146 seconds
[2024-03-23T20:17:05.502+0000] {processor.py:161} INFO - Started process (PID=10232) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:17:05.503+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:17:05.507+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:17:05.506+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:17:05.530+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:17:05.561+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:17:05.560+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:17:05.594+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:17:05.593+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:17:05.627+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.129 seconds
[2024-03-23T20:17:35.809+0000] {processor.py:161} INFO - Started process (PID=10248) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:17:35.811+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:17:35.818+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:17:35.817+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:17:35.856+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:17:35.912+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:17:35.912+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:17:35.939+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:17:35.939+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:17:35.971+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.169 seconds
[2024-03-23T20:18:06.019+0000] {processor.py:161} INFO - Started process (PID=10264) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:18:06.020+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:18:06.023+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:18:06.022+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:18:06.058+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:18:06.098+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:18:06.098+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:18:06.129+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:18:06.128+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:18:06.159+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.148 seconds
[2024-03-23T20:18:36.283+0000] {processor.py:161} INFO - Started process (PID=10280) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:18:36.286+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:18:36.293+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:18:36.291+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:18:36.328+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:18:36.365+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:18:36.364+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:18:36.427+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:18:36.427+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:18:36.470+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.193 seconds
[2024-03-23T20:19:06.543+0000] {processor.py:161} INFO - Started process (PID=10296) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:19:06.545+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:19:06.548+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:19:06.547+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:19:06.565+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:19:06.591+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:19:06.591+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:19:06.622+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:19:06.622+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:19:06.652+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.112 seconds
[2024-03-23T20:19:36.840+0000] {processor.py:161} INFO - Started process (PID=10312) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:19:36.841+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:19:36.844+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:19:36.844+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:19:36.861+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:19:36.896+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:19:36.895+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:19:36.951+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:19:36.950+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:19:36.985+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.148 seconds
[2024-03-23T20:20:07.053+0000] {processor.py:161} INFO - Started process (PID=10329) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:20:07.055+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:20:07.062+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:20:07.060+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:20:07.093+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:20:07.130+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:20:07.129+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:20:07.163+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:20:07.163+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:20:07.199+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.153 seconds
[2024-03-23T20:20:37.389+0000] {processor.py:161} INFO - Started process (PID=10345) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:20:37.390+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:20:37.394+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:20:37.393+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:20:37.413+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:20:37.442+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:20:37.441+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:20:37.468+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:20:37.468+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:20:37.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.115 seconds
[2024-03-23T20:21:07.628+0000] {processor.py:161} INFO - Started process (PID=10361) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:21:07.630+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:21:07.635+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:21:07.634+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:21:07.672+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:21:07.724+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:21:07.723+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:21:07.786+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:21:07.786+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:21:07.822+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.197 seconds
[2024-03-23T20:21:37.936+0000] {processor.py:161} INFO - Started process (PID=10377) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:21:37.939+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:21:37.944+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:21:37.943+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:21:37.981+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:21:38.040+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:21:38.039+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:21:38.067+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:21:38.066+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:21:38.101+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.172 seconds
[2024-03-23T20:22:08.222+0000] {processor.py:161} INFO - Started process (PID=10393) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:22:08.224+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:22:08.230+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:22:08.229+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:22:08.251+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:22:08.306+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:22:08.305+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:22:08.352+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:22:08.352+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:22:08.391+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.177 seconds
[2024-03-23T20:22:38.483+0000] {processor.py:161} INFO - Started process (PID=10409) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:22:38.484+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:22:38.488+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:22:38.488+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:22:38.513+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:22:38.549+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:22:38.548+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:22:38.588+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:22:38.587+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:22:38.632+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.153 seconds
[2024-03-23T20:23:08.680+0000] {processor.py:161} INFO - Started process (PID=10425) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:23:08.683+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:23:08.688+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:23:08.687+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:23:08.724+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:23:08.768+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:23:08.768+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:23:08.796+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:23:08.795+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:23:08.834+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.162 seconds
[2024-03-23T20:23:38.951+0000] {processor.py:161} INFO - Started process (PID=10440) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:23:38.954+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:23:38.959+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:23:38.958+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:23:38.997+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:23:39.037+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:23:39.036+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:23:39.071+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:23:39.071+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:23:39.109+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.166 seconds
[2024-03-23T20:24:09.195+0000] {processor.py:161} INFO - Started process (PID=10456) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:24:09.197+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:24:09.202+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:24:09.201+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:24:09.232+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:24:09.281+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:24:09.280+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:24:09.310+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:24:09.310+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:24:09.337+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.146 seconds
[2024-03-23T20:24:39.525+0000] {processor.py:161} INFO - Started process (PID=10472) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:24:39.527+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:24:39.533+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:24:39.531+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:24:39.571+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:24:39.625+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:24:39.624+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:24:39.674+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:24:39.673+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:24:39.717+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.199 seconds
[2024-03-23T20:25:09.815+0000] {processor.py:161} INFO - Started process (PID=10488) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:25:09.817+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:25:09.821+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:25:09.820+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:25:09.851+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:25:09.907+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:25:09.906+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:25:09.939+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:25:09.939+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:25:09.975+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.162 seconds
[2024-03-23T20:25:40.077+0000] {processor.py:161} INFO - Started process (PID=10504) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:25:40.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:25:40.089+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:25:40.088+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:25:40.128+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:25:40.174+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:25:40.171+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:25:40.214+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:25:40.214+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:25:40.262+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.195 seconds
[2024-03-23T20:26:10.333+0000] {processor.py:161} INFO - Started process (PID=10520) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:26:10.334+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:26:10.339+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:26:10.338+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:26:10.364+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:26:10.397+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:26:10.396+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:26:10.432+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:26:10.431+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:26:10.470+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.141 seconds
[2024-03-23T20:26:40.572+0000] {processor.py:161} INFO - Started process (PID=10535) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:26:40.575+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:26:40.582+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:26:40.580+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:26:40.618+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:26:40.670+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:26:40.669+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:26:40.699+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:26:40.699+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:26:40.733+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.168 seconds
[2024-03-23T20:27:10.813+0000] {processor.py:161} INFO - Started process (PID=10551) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:27:10.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:27:10.818+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:27:10.817+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:27:10.837+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:27:10.870+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:27:10.869+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:27:10.901+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:27:10.901+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:27:10.941+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.132 seconds
[2024-03-23T20:27:41.132+0000] {processor.py:161} INFO - Started process (PID=10567) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:27:41.133+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:27:41.137+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:27:41.136+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:27:41.156+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:27:41.190+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:27:41.189+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:27:41.223+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:27:41.223+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:27:41.259+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.131 seconds
[2024-03-23T20:28:11.438+0000] {processor.py:161} INFO - Started process (PID=10583) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:28:11.439+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:28:11.444+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:28:11.443+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:28:11.474+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:28:11.520+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:28:11.519+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:28:11.561+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:28:11.561+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:28:11.603+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.169 seconds
[2024-03-23T20:28:41.681+0000] {processor.py:161} INFO - Started process (PID=10599) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:28:41.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:28:41.685+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:28:41.684+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:28:41.707+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:28:41.739+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:28:41.738+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:28:41.770+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:28:41.770+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:28:41.808+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.131 seconds
[2024-03-23T20:29:11.879+0000] {processor.py:161} INFO - Started process (PID=10615) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:29:11.884+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:29:11.888+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:29:11.887+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:29:11.933+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:29:11.990+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:29:11.989+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:29:12.029+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:29:12.029+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:29:12.059+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.186 seconds
[2024-03-23T20:29:42.177+0000] {processor.py:161} INFO - Started process (PID=10631) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:29:42.180+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:29:42.188+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:29:42.187+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:29:42.268+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:29:42.401+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:29:42.398+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:29:42.480+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:29:42.480+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:29:42.513+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.345 seconds
[2024-03-23T20:30:13.404+0000] {processor.py:161} INFO - Started process (PID=10647) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:30:13.405+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:30:13.408+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:30:13.407+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:30:13.426+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:30:13.457+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:30:13.456+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:30:13.487+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:30:13.487+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:30:13.519+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.119 seconds
[2024-03-23T20:30:43.765+0000] {processor.py:161} INFO - Started process (PID=10663) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:30:43.766+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:30:43.770+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:30:43.769+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:30:43.791+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:30:43.823+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:30:43.822+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:30:43.852+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:30:43.852+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:30:43.879+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.117 seconds
[2024-03-23T20:31:13.996+0000] {processor.py:161} INFO - Started process (PID=10679) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:31:13.998+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:31:14.005+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:31:14.002+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:31:14.023+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:31:14.054+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:31:14.054+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:31:14.083+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:31:14.083+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:31:14.113+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.125 seconds
[2024-03-23T20:31:44.292+0000] {processor.py:161} INFO - Started process (PID=10695) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:31:44.294+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:31:44.299+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:31:44.298+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:31:44.317+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:31:44.346+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:31:44.345+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:31:44.374+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:31:44.374+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:31:44.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.121 seconds
[2024-03-23T20:32:14.487+0000] {processor.py:161} INFO - Started process (PID=10711) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:32:14.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:32:14.492+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:32:14.492+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:32:14.515+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:32:14.554+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:32:14.553+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:32:14.586+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:32:14.586+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:32:14.634+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.151 seconds
[2024-03-23T20:32:44.764+0000] {processor.py:161} INFO - Started process (PID=10727) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:32:44.766+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:32:44.772+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:32:44.771+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:32:44.809+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:32:44.842+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:32:44.841+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:32:44.869+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:32:44.869+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:32:44.904+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.149 seconds
[2024-03-23T20:33:15.015+0000] {processor.py:161} INFO - Started process (PID=10743) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:33:15.017+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:33:15.023+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:33:15.021+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:33:15.056+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:33:15.098+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:33:15.097+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:33:15.123+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:33:15.122+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:33:15.166+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.158 seconds
[2024-03-23T20:33:45.277+0000] {processor.py:161} INFO - Started process (PID=10759) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:33:45.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:33:45.285+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:33:45.283+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:33:45.319+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:33:45.363+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:33:45.363+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:33:45.391+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:33:45.391+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:33:45.413+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.144 seconds
[2024-03-23T20:34:15.625+0000] {processor.py:161} INFO - Started process (PID=10775) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:34:15.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:34:15.634+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:34:15.632+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:34:15.669+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:34:15.722+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:34:15.721+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:34:15.748+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:34:15.747+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:34:15.777+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.158 seconds
[2024-03-23T20:34:45.860+0000] {processor.py:161} INFO - Started process (PID=10791) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:34:45.862+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:34:45.868+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:34:45.867+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:34:45.890+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:34:45.934+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:34:45.934+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:34:45.963+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:34:45.963+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:34:45.995+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.142 seconds
[2024-03-23T20:35:16.199+0000] {processor.py:161} INFO - Started process (PID=10807) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:35:16.201+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:35:16.207+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:35:16.206+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:35:16.244+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:35:16.301+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:35:16.301+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:35:16.327+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:35:16.327+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:35:16.351+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.158 seconds
[2024-03-23T20:35:46.429+0000] {processor.py:161} INFO - Started process (PID=10823) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:35:46.431+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:35:46.438+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:35:46.437+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:35:46.473+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:35:46.524+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:35:46.523+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:35:46.556+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:35:46.555+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:35:46.587+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.165 seconds
[2024-03-23T20:36:16.808+0000] {processor.py:161} INFO - Started process (PID=10839) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:36:16.810+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:36:16.817+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:36:16.815+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:36:16.852+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:36:16.906+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:36:16.905+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:36:16.945+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:36:16.945+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:36:16.980+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.179 seconds
[2024-03-23T20:36:47.052+0000] {processor.py:161} INFO - Started process (PID=10855) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:36:47.054+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:36:47.060+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:36:47.059+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:36:47.098+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:36:47.153+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:36:47.152+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:36:47.189+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:36:47.189+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:36:47.217+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.172 seconds
[2024-03-23T20:37:17.312+0000] {processor.py:161} INFO - Started process (PID=10871) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:37:17.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:37:17.319+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:37:17.318+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:37:17.356+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:37:17.414+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:37:17.413+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:37:17.445+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:37:17.445+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:37:17.470+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.165 seconds
[2024-03-23T20:37:47.688+0000] {processor.py:161} INFO - Started process (PID=10887) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:37:47.690+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:37:47.696+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:37:47.694+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:37:47.734+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:37:47.791+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:37:47.791+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:37:47.843+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:37:47.843+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:37:47.879+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.199 seconds
[2024-03-23T20:38:17.934+0000] {processor.py:161} INFO - Started process (PID=10903) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:38:17.935+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:38:17.941+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:38:17.940+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:38:17.960+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:38:18.016+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:38:18.015+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:38:18.063+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:38:18.063+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:38:18.098+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.167 seconds
[2024-03-23T20:38:48.164+0000] {processor.py:161} INFO - Started process (PID=10919) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:38:48.167+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:38:48.172+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:38:48.171+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:38:48.210+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:38:48.264+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:38:48.263+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:38:48.306+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:38:48.306+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:38:48.339+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.182 seconds
[2024-03-23T20:39:18.436+0000] {processor.py:161} INFO - Started process (PID=10935) to work on /opt/airflow/dags/pipeline.py
[2024-03-23T20:39:18.438+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/pipeline.py for tasks to queue
[2024-03-23T20:39:18.444+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:39:18.443+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/pipeline.py
[2024-03-23T20:39:18.476+0000] {processor.py:840} INFO - DAG(s) 'etl_to_bigquery' retrieved from /opt/airflow/dags/pipeline.py
[2024-03-23T20:39:18.522+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:39:18.521+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-03-23T20:39:18.554+0000] {logging_mixin.py:188} INFO - [2024-03-23T20:39:18.553+0000] {dag.py:3823} INFO - Setting next_dagrun for etl_to_bigquery to 2024-03-22 00:00:00+00:00, run_after=2024-03-23 00:00:00+00:00
[2024-03-23T20:39:18.590+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/pipeline.py took 0.163 seconds
